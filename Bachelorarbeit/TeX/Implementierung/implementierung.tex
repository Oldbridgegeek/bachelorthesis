In Kapitel 3 haben wir uns zwei Möglichkeiten angeschaut die Pseudoinversen herzuleiten. In beiden Möglichkeiten fand sich das Kronecker-Produkt.
Um dieses effizient implementieren zu können, sollten wir uns Gedanken darüber machen, wie diese Struktur nutzbar ist.

In \cite{Teachlet} wird die effektive Berechnung der Massenmatrix mit einem Vektor multipliziert, vorgestellt. Dafür wird die Tensorstruktur der Massenmatrix ausgenutzt. Diese Methodik sollten wir uns kurz vor Augen führen und daraufaufbauend, um einige eigene Gedanken erweitern, um sie für unsere Anwendung zugänglich zu machen.

\subsection{Effizientes Matrix-Vektor Produkt }
In \cite{Teachlet} wird eine Strategie vorgestellt, um ein Matrix-Vektor Produkt mit Kronecker Produkt Matrizen $z=(\mathcal{B} \otimes \mathcal{A})y$ effektiv zu berechnen.

Sei $\mathcal{A} \in \mathbb{R}^{m \times n}$ und $\mathcal{B} \in \mathbb{R}^{p \times q}$. Das Kronecker Produkt dieser Matrizen kann man schreiben als
\begin{equation*}
\mathcal{B} \times \mathcal{A} =
\begin{pmatrix}
b_{11}\mathcal{A} & \dots  & b_{1q}\mathcal{A} \\
\vdots & \ddots & \vdots \\
b_{p1}\mathcal{A} & \dots & b_{pq}\mathcal{A} \\
\end{pmatrix}.
\end{equation*}

Wir sehen sich wiederholende Strukturen von $\mathcal{A}$. Genau diese wollen wir uns zunutze machen. Nehmen wir an $y$ sei geordnet in der Indexierung.
\begin{equation*}
y = (y_1,y_2,\dots,y_n,\dots,\dots,y_{(q-1)n+1},y_{(q-1)n+2},\dots,y_{qn})^T
\end{equation*}

Wir denken uns die Faktoren $b_{ij}$, die mit $\mathcal{A}$ multipliziert werden, weg. Definiere $y^{(1)}=(y_1,y_2,\dots,y_n)^T$.

\begin{equation*}
w^{(1)}=
\begin{pmatrix}
w_1 \\ \vdots \\ w_m 
\end{pmatrix}
=
\begin{pmatrix}
a_{11} & \dots & a_{1n} \\
\vdots & \dots & \vdots \\
a_{m1} & \dots & a_{mn} \\
\end{pmatrix}
\begin{pmatrix}
y_1 \\ \vdots \\ y_n
\end{pmatrix}
= \mathcal{A}y^{(1)}
\end{equation*}

Auf ähnliche Weise können wir $y^{(2)}=(y_{n+1},\dots,y_{2n})^{T}$ definieren und
\begin{equation*}
w^{(2)} = \mathcal{A}y^{(2)}.
\end{equation*}

Wir führen dies weiter und erhalten

\begin{equation*}
w=( (w^{(1)})^T , \dots, (w^{(q)})^T)^T \in \mathbb{R}^{mq}.
\end{equation*}
Es müssen die Informationen der Matrix $\mathcal{B}$ noch eingebracht werden. Dazu berechnen wir wie in \cite{Teachlet} vorgeschlagen $z_i$ mit

\begin{equation*}
z^{(k)}_j = \sum_{i=1}^{q} b_{ki} w_j^{(i)}
\end{equation*}

Der komplette Algorithmus ist nachfolgend dargestellt.
\begin{mdframed}[backgroundcolor=blue!3] 
\begin{algorithmic}
\For {i=1 < q }
	\For {j= 1 < m }
			\State $w^{(i)}_{j} = y^{(i)}_1 a_{j1} + \dots + y^{(i)}_n a_{jn}$
	\EndFor
\EndFor
\For {k=1 < n }
	\For {j= 1 < p }
			\State $z^{(k)}_j := w^{(1)}_j b_{k1} + \dots + w^{(q)}_j b_{kq}$
	\EndFor
\EndFor

\end{algorithmic}
\end{mdframed}


Nehmen wir für die triviale Berechnung der Komplexität an $n=m=p=q$.
Mit Hilfe dieses Algorithmus haben wir die Komplexität des Auswertens der Gleichung von $2m^4$ Operationen auf $4m^3$ Operationen reduziert.

Dieser Algorithmus ist optimal für das effektive Berechnen von
\begin{equation} \label{eq:vmult}
v=M^+u=[\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T]^+ \otimes [\mathcal{W}_N^{1D} (\mathcal{N}_{1D})^T]^+ u \,.
\end{equation}

Den Algorithmus kann man ebenfalls als Matrix-Produkt darstellen. Dazu ist es notwendig, die Vektoren $u$ und $v$ in Matrizen zu überführen. Dies erfolgt über unsere Index-Transformation $p(\cdot)$. Seien $V$ die zu $v$, und $U$ die zu $u$ korrespondierenden Matrizen. Dann kann man (\ref{eq:vmult}) darstellen als

\begin{equation}
V= [\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T]^+ U ([\mathcal{W}_N^{1D} (\mathcal{N}_{1D})^T]^+)^T.
\end{equation}

Diese Überführung wird später für die Berechnung der Laplace Bilinearform benötigt.

Für den zweiten Ansatz über die HOSVD werden wir eine erweiterte Form des Algorithmus benötigen, da wir mehrere Kronecker Produkte erhalten. Außerdem kann der erweiterte Algorithmus dann auch für dreidimensionale Ansatzfunktionen beim ersten Ansatz verwendet werden.


\subsubsection{Erweiterung}
Wir wollen den Algorithmus erweitern für die Berechnung von $z=(\mathcal{C} \otimes \mathcal{B} \otimes \mathcal{A})v$ mit $\mathcal{A} \in \mathbb{R}^{N \times N}$, $\mathcal{B} \in \mathbb{R}^{N \times N}$, $\mathcal{C} \in \mathbb{R}^{N \times N}$ und $v \in \mathbb{R}^{N^3}$.


\begin{equation} z=
\begin{pmatrix}
c_{11} b_{11} \mathcal{A} & \dots  & c_{11} b_{1N} \mathcal{A} & \dots & \dots & c_{1N}b_{11}\mathcal{A} & \dots & c_{1N}b_{1N}\mathcal{A}  \\

\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{11} b_{N 1} \mathcal{A} & \dots  & c_{11} b_{N N} \mathcal{A} & \dots & \dots & c_{1 N}b_{N 1}\mathcal{A} & \dots & c_{1 N}b_{N N}\mathcal{A}  \\
c_{21} b_{11} \mathcal{A} & \dots  & c_{21} b_{1N} \mathcal{A} & \dots & \dots & c_{2N}b_{11}\mathcal{A} & \dots & c_{2 N}b_{1 N}\mathcal{A}  \\
\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{N 1} b_{N 1} \mathcal{A} & \dots  & c_{N 1} b_{N N} \mathcal{A} & \dots & \dots & c_{N N}b_{N 1}\mathcal{A} & \dots & c_{N N}b_{N N}\mathcal{A}  \\
\end{pmatrix} \, \, v
\end{equation}

Wir sehen hier zwei sich wiederholende Strukturen, die wir ausnutzen wollen um Operationen zu sparen.

\begin{equation} \label{eq:matrix} z=
\begin{pmatrix}
c_{11} \textcolor{green}{b}_{11} \textcolor{red}{\mathcal{A}} & \dots  & c_{11} \textcolor{green}{b}_{1N} \textcolor{red}{\mathcal{A}} & \dots & \dots & c_{1N}\textcolor{green}{b}_{11}\textcolor{red}{\mathcal{A}} & \dots & c_{1N}\textcolor{green}{b}_{1N}\textcolor{red}{\mathcal{A}}  \\

\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{11} \textcolor{green}{b}_{N 1} \textcolor{red}{\mathcal{A}} & \dots  & c_{11} \textcolor{green}{b}_{N N} \textcolor{red}{\mathcal{A}} & \dots & \dots & c_{1 N}\textcolor{green}{b}_{N 1}\textcolor{red}{\mathcal{A}} & \dots & c_{1 N}\textcolor{green}{b}_{N N}\textcolor{red}{\mathcal{A}}  \\
c_{21} \textcolor{green}{b}_{11} \textcolor{red}{\mathcal{A}} & \dots  & c_{21} \textcolor{green}{b}_{1N} \textcolor{red}{\mathcal{A}} & \dots & \dots & c_{2N}\textcolor{green}{b}_{11}\textcolor{red}{\mathcal{A}} & \dots & c_{2 N}\textcolor{green}{b}_{1 N}\textcolor{red}{\mathcal{A}}  \\
\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{N 1} \textcolor{green}{b}_{N 1} \textcolor{red}{\mathcal{A}} & \dots  & c_{N 1} \textcolor{green}{b}_{N N} \textcolor{red}{\mathcal{A}} & \dots & \dots & c_{N N}\textcolor{green}{b}_{N 1}\textcolor{red}{\mathcal{A}} & \dots & c_{N N}\textcolor{green}{b}_{N N}\textcolor{red}{\mathcal{A}}  \\
\end{pmatrix} \, \, v
\end{equation}

Den Vektor $v$ können wir zu einem Tensor $\mathcal{V} \in \mathbb{R}^{N \times N \times N} $ umdefinieren, damit dieser handlicher für unsere Anwendung wird. Der erste Index repräsentiert, in welchem Spalteneintrag von $\mathcal{C}$, der zweite Index, in welchem Spalteneintrag von $\mathcal{B}$ und der dritte Index, in welchem Spalteneintrag von $\mathcal{A}$ wir uns befinden. Dies kann man als Index-Transformation ansehen, die bestimmte Einträge von $v$ eindeutig auf Tensorelemente abbildet. 
Wir schauen uns zuerst die einzelnen Einträge von $z$ an. 

Dadurch können wir den ersten Eintrag von $z$ darstellen durch
\begin{equation*} \label{eq:zold}
\begin{aligned}
z_{1} = \mathcal{V}(1,1,1) c_{11} \textcolor{green}{b_{11}} \textcolor{red}{a_{11}}+ \dots +  \mathcal{V}(1,1,N) c_{11} \textcolor{green}{b_{11}} \textcolor{red}{a_{1N}} + \dots  +  \mathcal{V}(1,N,1)c_{11} 
\textcolor{green}{b_{1N}} \textcolor{red}{a_{11}} \\ + \dots +  \mathcal{V}(N,1,1) c_{1N} \textcolor{green}{b_{11}} \textcolor{red}{a_{11}} + \dots +  \mathcal{V}(N,N,N) c_{1N} \textcolor{green}{b_{1N}} \textcolor{red}{a_{1N}}.
\end{aligned}
\end{equation*}
Definiere $$w_{\textcolor{red}{1}}(i,j) :=\mathcal{V}(i,j,1) a_{\textcolor{red}{1}1}+\dots+\mathcal{V}(i,j,N) a_{\textcolor{red}{1}N}.$$ Dann erhalten wir
\begin{equation*}
\begin{aligned}
z_{1}= w_1(1,1) c_{11} b_{11} + \dots +   w_1(1,N) c_{11} b_{1N} + \dots + w_1(n,1) c_{1N} b_{11}  + \dots +  w_1(N,N) c_{1N} b_{1N}.
\end{aligned}
\end{equation*}

Damit haben wir uns die sich wiederholende Struktur von der Matrix $\mathcal{A}$ zunutze gemacht. Im nächsten Schritt machen wir uns die sich wiederholende Struktur von $\mathcal{B}$ zunutze.
Wir definieren hierfür $$\mathcal{W}_{\textcolor{red}{1},k} (i):= w_k(i,1) b_{\textcolor{red}{1}1} + \dots + w_k(i,N) b_{\textcolor{red}{1}N}.$$ Damit erhalten wir
\begin{equation*} \label{eq:znew}
\begin{aligned}
z_{1}= \mathcal{W}_{1,1}(1) c_{11}  + \dots +  \mathcal{W}_{1,1}(N) c_{1N}.
\end{aligned}
\end{equation*}

Wir formen $z$ genau so wie wir auch für $v$ verfahren sind und erhalten dafür den Tensor $\mathcal{Z}$. Damit erhalten wir für einen beliebigen Eintrag von $z$ folgende Formel
\begin{equation*}
\mathcal{Z}(i,j,k) = \mathcal{W}_{j,k}(1) c_{i1}  + \dots +  \mathcal{W}_{j,k}(N) c_{iN} \, ,
\end{equation*}
wobei j und k den Zeilen jeweils in den Matrizen $\mathcal{B}$ und $\mathcal{C}$ entsprechen. \\
Der komplette Algorithmus ist nachfolgend dargestellt.
\newpage
\begin{mdframed}[backgroundcolor=blue!3] 
\begin{algorithmic}
\For {k=1 < N }
	\For {i= 1 < N }
		\For {j= 1 < N }
			\State $w_{k}(i,j) = \mathcal{V}(i,j,1)a_{k1} + \dots + \mathcal{V}(i,j,N)a_{kN}$
		\EndFor
	\EndFor
\EndFor
\For {k=1 < N }
	\For {i= 1 < N }
		\For {j= 1 < N }
			\State $\mathcal{W}_{i,j} (k):= w_j(k,1) b_{i1} + \dots + w_j(k,N) b_{iN}$
		\EndFor
	\EndFor
\EndFor
\For {k=1 < N }
	\For {i= 1 < N }
		\For {j= 1 < N }
			\State $\mathcal{Z}(i,j,k) = \mathcal{W}_{j,k}(1) c_{i1}  + \dots +  \mathcal{W}_{j,k}(N) c_{iN}$ 
		\EndFor
	\EndFor
\EndFor
\end{algorithmic}
\end{mdframed}

 
Wir haben in (\ref{eq:matrix}) eine Matrix-Vektor Multiplikation von einer Matrix der Größe $N^3 \times N^3$. Dementsprechend hätten wir $N^{6}$ Multiplikationen und $N^6$ Additionen. Dies entspricht $2N^6$ elementaren Operationen. Die Komplexität des vorgeschlagenen Algorithmus reduziert die Operationen auf $3N^{4}$ Multiplikationen und genauso viele Additionen. Somit haben wir insgesamt $6N^4$ Operationen. Das ist, vor allem für großes $N$, eine beträchtliche Reduktion.
\newline

\newpage
\subsection{Tensorstruktur}

Wie können wir diese Algorithmen für die in Kapitel 3 angesprochenen Strategien verwenden?

In diesem Unterkapitel schauen wir uns die Anwendung der hergeleiteten Algorithmen auf die Tensorstruktur unserer Bilinearformen an. Zudem wollen wir eine Komplexitätsanalyse für jeden der Ansätze angeben. Zuerst schauen wir uns das Matrix-Vektor Produkt mit der Pseudoinversen der Massenmatrix und einem beliebigen Vektor $u$ an. Danach kommen wir zur Lösung des Problems mit der Laplace Bilinearform. \\
\textbf{Massenmatrix} 
\begin{equation*}
M^{+}u =  [(\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T)^+ \otimes (\mathcal{W}_N^{1D} (\mathcal{N}_{1D})^T)^+]u.
\end{equation*}



Wie komplex ist es, die Pseudoinverse der Matrix $(\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T) \in \mathbb{R}^{N \times N}$ zu berechnen, wobei $N$ die Anzahl der Freiheitsgrade in einer Dimension ist.

Wir können als Ansatz eine Singulärwertzerlegung wählen, aus der zuvor Pseudoinverse hergeleitet werden muss.
Allgemein gilt
\begin{equation*}
M\,=\,U\Sigma V^{T}
\end{equation*}
für eine eine $N \times N$-Matrix $M$ mit Rang $r$, wobei
$U$ eine orthogonale $N\times N$-Matrix, $V^{T}$ die Transponierte einer orthogonalen $N \times N$-Matrix $V$ und
$\Sigma$  eine reelle  $N\times N$-Diagonalmatrix ist.

Die daraus hergeleitete Pseudoinverse ergibt
\begin{equation*}
M^{+}=V\Sigma ^{+}U^{T}.
\end{equation*}

Die Komplexität für die Herleitung der Pseudoinversen aus der Singulärwertzerlegung ist vernachlässigbar. Für die Berechnung von $\Sigma^+$ haben wir $r$ Operationen, da in $\Sigma$ in der Diagonalen $r$ Einträge stehen, die wir nur invertieren müssen. Ebenfalls ist die Berechnung von $U^{T}$ ist vernachlässigbar, da man bei der Berechnung der Singulärwertzerlegung direkt $U^{T}$ speichern kann anstatt $U$.
Somit bekommen wir mit der Singulärwertzerlegung einer Matrix $M \in \mathbb{R}^{N \times N}$ die Pseudoinverse mit einer Komplexität von $O(N^3)$ \cite[2]{SVD}. Man könnte an dieser Stelle auch approximative Verfahren wählen und versuchen an Operationen zu sparen. Ich verweise hier auf \cite{SVD} für die nähere Betrachtung von schnelleren Singulärwertzerlegungen.

Für die Berechnung der Inversen kann man mit dem Gauss Algorithmus. Der Gauss Algorithmus gibt uns eine Komplexität von $O(N^3)$. 
Der Gauss Algorithmus ergibt entsprechend einen Sinn, wenn $(\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T)$ invertierbar ist.

\begin{Bemerkung} (Invertierbarkeit) \\ \label{bem:unit}
Es seien $\mathcal{W}_N^{1D} \in \mathbb{R}^{N \times N}$ und $(\mathcal{N}^{1D})^T \in \mathbb{R}^{N \times N}$ Matrizen, die wie gewohnt definiert sind. Dann gilt, dass $(\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T) \in \mathbb{R}^{N \times N}$ invertierbar ist.

\begin{proof}

Man kann durch einfache Basistransformation erreichen, \\ dass $(\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T) \in \mathbb{R}^{n \times n}$  die Einheitsmatrix ist. Es seien $x=\{x_1,\dots,x_N\}$ die Stützstellen und $w=\{w_1,\dots,w_N\}$ die Gewichte der Quadratur.

Wir wählen 
\begin{equation*}
\varphi^{1D}_i (x_k) = \dfrac{1}{ \sqrt{w_i} } l_i (x_k) = 
\begin{cases}
\dfrac{1}{ \sqrt{w_i} } \, \text{ , wenn } i=k  \\
0  \, \, \, \, \, \, \, \, \, \, \, \text{ , sonst }
\end{cases}
\end{equation*}

als neue Basis.

Die Funktion $l_i(x_k)$ bezeichnet das $i-te$ Lagrange Polynom ist.
Es folgt 
\begin{equation*}
\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T = \mathcal{N}^{1D} \mathcal{W}^{1D} (\mathcal{N}^{1D})^T = I_N.
\end{equation*}

Da $\mathcal{N}^{1D}$ eine Diagonalmatrix ist, mit den Diagonaleinträgen $\dfrac{1}{ \sqrt{w_i} }$, gilt $$\mathcal{N}^{1D}=(\mathcal{N}^{1D})^T.$$ Weiterhin ist $\mathcal{W}^{1D}$ auch eine Diagonalmatrix mit den Einträgen $w_i$ in der Diagonalen. In der Ergebnismatrix steht folglich in der Diagonalen 
\begin{equation*}
(\dfrac{1}{ \sqrt{w_i} })^2  w_i = 1 \, .
\end{equation*}
 
\end{proof}
\end{Bemerkung}
\newpage
Insgesamt erhalten wir für die Berechnung der Inversen beziehungsweise der Pseudoinversen folgende Komplexität:
\begin{itemize}
\item Um das Matrix-Vektor Produkt effizient zu berechnen nutzen wir den Algorithmus für die Berechnung von $z=(\mathcal{B} \otimes \mathcal{A})y$ aus Kapitel 4.1. und erhalten eine Komplexität von $4N^3$.

\item Matrizenmultiplikation ist kubisch.

\item Berechnung der Pseudoinversen/Inversen liegt in $O(N^3)$.

\end{itemize}

Insgesamt haben wir eine Komplexität von $O(4N^3)+O(N^3)+O(N^3)=O(6N^3)$.
Wie weitergehend an Komplexität gespart werden kann und wo wir anknüpfen können, um effizienter zu werden, wird in Kapitel 5 diskutiert.
Wir kommen zu dem Problem mit der Laplace Bilinearform.
\vspace*{1cm}
\textbf{Laplace Bilinearform}
\begin{equation*}
y=V^{+}u = \big{(} [(\widehat{\mathcal{W}}_N^{1D} (\widehat{\mathcal{N}}^{1D})^T] \otimes [\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T]  +  [\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T] \otimes [\widehat{\mathcal{W}}_N^{1D} (\widehat{\mathcal{N}}^{1D})^T]  \big{)}^+ u
\end{equation*}

Wir wollen dies weiter vereinfachen. Definiere $A=[(\widehat{\mathcal{W}}_N^{1D} (\widehat{\mathcal{N}}^{1D})^T]$. Wähle Basis so, dass $\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T  = I_N$. Dies ist, wie in Beweis für Bemerkung (\ref{bem:unit}) möglich.

Wir erhalten folgende vereinfachte Form.
\begin{equation*} \label{eq:easy}
Vu =[(A \otimes I_) + (I_n \otimes A)]u.
\end{equation*}

Mit der in Kapital 4.1 besprochenen Strategie, können wir den Vektoren $u$ und $y$ in Matrizen überführen, sodass wir (\ref{eq:easy}) äquivalent umformen können zu
\begin{equation*} \label{eq:easy}
Y=V^{+}U =[(A \otimes I_N) + (I_N \otimes A)]U,
\end{equation*}
wobei $U$  und $Y$ definiert sind durch $U_{ij}=u(q_i,q_j)$ und $Y_{ij}=Y(q_i,q_j)$.

Dies können wir weiter vereinfachen.
\begin{equation*} \label{eq:easy}
V=(A \otimes I_N)U + (I_N \otimes A)U = AUI_n + I_n U A^T = AU + UA^T
\end{equation*}

Dies ist die Lyapunow Gleichung aus der Stabilitätstheorie. Zur Lösung der Lyapunow Gleichung gibt es zahlreiche Untersuchungen. Die Behandlung dieser Gleichung würde den Rahmen dieser Bachelorarbeit sprengen. Ich empfehle als Literatur für diese Untersuchungen \cite{Lyapunov}.

Die Lösung der Gleichung ist gegeben durch
\begin{equation*}
U = \int\limits_{0}^{\infty} e^{A \tau} (-V) e^{A^T \tau} d\tau.
\end{equation*}

Die naive Berechnung der Lösung dieser Gleichung erfolgt mit einer Komplexität von $O(N^6)$. Der Bartels-Stewart Algorithmus \cite{Bartels} liefert uns eine Komplexität von $O(N^3)$. 

\subsection{Singulärwertzerlegung höherer Ordnung}
Nun haben wir uns mit Hilfe der HOSVD eine Herleitung für die Pseudoinverse erarbeitet. Folgend geht es um die effiziente Berechnung dieser Formel. Wir können uns zwei verschiedene Ansätze mit der HOSVD anschauen. Den ersten Ansatz nennen wir den naiven Ansatz und dieser erfolgt über die einfache Form der HOSVD mit dem $n-mode$ Produkt. Im zweiten Ansatz schauen wir uns die entfaltete HOSVD an. Dort erkennen wir eine Kronecker Produkt Struktur. Dazu wollen wir uns die in Kapitel 4.1.1. hergeleitete Strategie zunutze machen, für die effektive Berechnung von den zwei Kronecker Produkten mit einem Vektor.
Es sei $\mathscr{X} \in \mathbb{R}^{I_1 \times \dots \times I_{4}}$ ein Tensor. Der Tensor $\mathscr{X}$ könnte der Massetensor sein oder der Laplace Bilinearform Tensor. Da die Massematrix und die Elementsteifigkeitsmatrix quadratisch sind, gilt $N:= I_1 = \dots = I_4$.
Die Formel für die Pseudoinverse lautet

\begin{equation} \label{eq:pinv}
\mathscr{X}^{+} = \mathscr{G}^{+} \times_{n=1}^{4} U^{ (n) ^{T} },
\end{equation}

wobei $\mathscr{G} \in \mathbb{R}^{N \times N \times N \times N}$ und $U^{(n)} \in \mathbb{R}^{N \times N}$.

Wie aufwändig ist es, die HOSVD zu berechnen?
In Bemerkung (\ref{hosvd}) haben wir den Algorithmus für die Berechnung der HOSVD angegeben. 
\begin{enumerate}
\item Für den ersten Schritt des Algorithmus müssen wir alle $mode-k$ Entfaltungen berechnen. Dies impliziert eine Ordnung jedes Elements des Tensors in ein Element der Ergebnismatrix. Damit haben wir genau so viele Zuweisungen, wie die Anzahl der Elemente des Tensors. In unserem Fall wären das $4N^4$ Operationen für alle modes. 

\item Die Singulärwertzerlegung für die Entfaltungen gilt es im zweiten Schritt zu berechnen. Eine Entfaltung ist eine Matrix der Größe $N \times N^3$. Dementsprechend würde eine Singulärwertzerlegung für diese Matrizen in der Komplexitätsklasse $O(N^5)$ liegen. Wir müssen dies für vier Matrizen durchführen und erhalten $O(4N^5)$.

\item Im dritten Schritt wollen wir den Kerntensor berechnen. Dazu müssen wir $n-mode$ Produkte berechnen. Ein $n-mode$ Produkt hat $N^5$ Additionen und genauso viele Multiplikationen. Davon müssen wir vier berechnen. Insgesamt haben wir eine Komplexität von $8N^5$ Rechenoperationen.

\end{enumerate}

Insgesamt ergibt sich eine Komplexität von $O(4N^4  + 12N^5)$. 
Die Berechnung der orthogonal Matrizen für die HOSVD zeigt, dass alle Matrizen gleich sind. Dementsprechend müssen wir im zweiten Schritt die Singulärwertzerlegung nur einmal berechnen. Außerdem müssen wir nur eine Entfaltung im ersten Schritt rechnen. Damit reduziert sich die Komplexität zu $O(N^4  + 9N^5)$. \\

Wir wollen aus der HOSVD nun die Pseudoinverse errechnen. Dies geschieht, indem wir die Transponierte der Faktormatrizen ausrechnen und den Kerntensor invertieren. Die Faktormatrizen können wir direkt in transponierter Form speichern. Um den Kerntensor zu invertieren müssen wir durch den Tensor iterieren und alle Elemente kleiner $\epsilon$ auf 0 setzen und alle Elemente größer oder gleich $\epsilon$ invertieren.
Dies ergibt eine Komplexität von $O(N^4)$, da der Tensor von Ordnung 4 ist.

Im nächsten Schritt wollen wir einen beliebigen Vektor $u \in \mathbb{R}^{N^3}$ an die HOSVD multiplizieren.

\subsubsection{Naiver Ansatz}
Wir schauen uns zuerst den naiven Ansatz an. Dafür benötigen wir jedoch zusätzlich ein geeignetes Tensor-Vektor Produkt. 
Dazu schauen wir uns das Matrix-Vektor Produkt an. Sei $A \in \mathbb{R}^{m \times n}$ und $x \in \mathbb{R}^{n}$. Dann ist $y=Ax \in \mathbb{R}^{m}$ definiert durch
\begin{equation*}
y_i = \sum\limits_{j=1}^{n} a_{ij} x_j.
\end{equation*}
Durch die Index-Transformation $p(\cdot)$ können wir dies für Tensoren herleiten.

\begin{Definition} (Tensor-Vektor Produkt) \\
Es sei $\mathcal{A} \in \mathbb{R}^{N \times N \times N \times N}$ und $U \in \mathbb{R}^{N \times N}$.
Dann ist $Y=tvp(\mathcal{A},U)$ gegeben durch

\begin{equation*}
Y_{i_1 \, i_2} = \sum\limits_{j_1=1}^{N} \sum\limits_{j_2=1}^{N} \mathcal{A}_{i_1 i_2 j_1 j_2} U_{j_1 j_2}
\end{equation*}

\end{Definition}

Nun können wir uns den naiven Ansatz anschauen, der wie folgt gegeben ist
\begin{equation} \label{eq:pinv}
tvp(\mathscr{X}^{+},U) = tvp(\mathscr{G}^{+} \times_{n=1}^{4} U^{ (n) ^{T} },U) ,
\end{equation} 
mit $U \in \mathbb{R}^{N \times N}$. Das ist das Matrix-Vektor Produkt in Tensorform. 

Die Komplexität für die Berechnung ist gegeben durch $N^4$. Denn wir haben für die Berechnung eines einzelnen Elementes $Y_{i_1 i_2}$ zwei Schleifen die bis $N$ durchlaufen. Insgesamt haben wir $N^2$ Elemente. Dies ergibt $N^4$ Operationen. \\
Mit der Berechnung der HOSVD, das Invertieren des Kerntensors und die Berechnung der $n-mode$ Produkte ergibt dieser Ansatz eine Komplexität von $O(3N^4 + 17N^5)$. \\

\subsubsection{Entfaltete HOSVD}
Wir wollen nun einen anderen Ansatz wählen und nutzen, dass wir die HOSVD in entfalteter Form betrachten können.
Man kann (\ref{eq:pinv}) wie in (\ref{eq:tensortensor}) äquivalent umformen zu

\begin{equation}
\begin{aligned}
\mathscr{X}^{+}_{(n)}  &= U^{ (n) ^{T} }  \mathscr{G}^{+}_{(n)} ( U^{ (4) ^{T} } \otimes \dots \otimes U^{ (n+1) ^{T} } \otimes U^{ (n-1) ^{T} } \otimes \dots \otimes U^{ (1) ^{T} })^{T} \\ \iff
\mathscr{X}^{+}_{(n)} &= U^{ (n)  }  \mathscr{G}^{+}_{(n)} ( U^{ (4)  } \otimes \dots \otimes U^{ (n+1) } \otimes U^{ (n-1) } \otimes \dots \otimes U^{ (1) }).
\end{aligned}
\end{equation}

Die Äquivalenz folgt mit Lemma (\ref{lemma:transpose})

\begin{equation} \label{eq:pinvv}
\begin{aligned}
\mathscr{X}^{+}_{(n)}v&= U^{ (n)  }  \mathscr{G}^{+}_{(n)} ( U^{ (4)  } \otimes \dots \otimes U^{ (n+1) } \otimes U^{ (n-1) } \otimes \dots \otimes U^{ (1) }) u.
\end{aligned}
\end{equation}

Wir führen die Variablen $N_i$ ein mit $N_i \neq n$. Damit können wir (\ref{eq:pinv}) reduzieren auf

\begin{equation} \label{eq:pinvcase}
\begin{aligned}
\mathscr{X}^{+}_{(n)} u&= \underbrace{\underbrace{U^{ (n) }  \mathscr{G}^{+}_{(n)}}_{K_1} \underbrace{( U^{ (N_{1})  } \otimes U^{ (N_{2})}  \otimes U^{ (N_{3}) }) u}_{K_2}}_{K_3} \,.
\end{aligned}
\end{equation}
 

Wir erkennen, dass wir die Methodik nutzen können, die wir entwickelt haben um $K_2$ effizient zu berechnen. Dies ist möglich mit einer Komplexität von $O(6N^4)$.

Das Invertieren des Kerntensors ist wie bereits veranschaulicht von der Komplexität $O(N^4)$.
Es folgt eine letzte Matrix-Matrix Multiplikation $U^{ (n) ^{T} }  \mathscr{G}^{+}_{(n)}$. Hierbei hat die Matrix $ \mathscr{G}^{+}_{(n)}$ die Größe $N \times N^3$ und die Matrix  $U^{ (n) ^{T} }$ die Größe $N \times N$.  Wir haben bei der Multiplikation dieser beiden Matrizen $N^5$ Multiplikationen und genauso viele Additionen. Dann können wir die Komplexitätsklassen durch $O(N^5)$ angeben, doch dank der Struktur des Kerntensors reduziert sich die Komplexität auf $O(N^4)$. Das liegt daran, dass der Tensor $ \mathscr{G}^{+}$ super-diagonal ist. Damit müssen wir also pro Element der Ergebnismatrix nicht die Spalte mal Zeile rechnen, sondern einfach nur das Diagonalelement mal ein Element der Matrix. 

Nun gilt es das Ergebnis von $K_1$ und das Ergebnis von $K_2$ durch ein Matrix-Matrix Produkt zusammenzurechnen. Die Matrix $K_1$ hat die Größe $N \times N^3$ und $K_2$ ist $N^3 \times N^3$ groß. 
Die Anzahl der Operationen für die Berechnung von $K_3$ liegt also in $O(2N^{5})$.

Insgesamt haben wir
\begin{itemize}
\item $O(N^4)$ für die Berechnung von $K_1$
\item $O(6N^4)$ für die Berechnung von $K_2$
\item $O(2N^{5})$ für die Berechnung von $K_3$
\end{itemize}

Insgesamt ergibt sich eine Belastung von $O(9N^4+11N^{5}) \in O(N^5)$. \\ Dieser Ansatz ist zwar effizienter als der Naive, aber nicht annähernd so gut wie der Tensorstruktur Ansatz. Wie können wir unsere Effizienz weiter steigern?

Wir können uns eine trunkierte Singulärwertzerlegung höherer Ordnung  wagen oder eine andere Alternative finden. Folgende Möglichkeiten zeigen sich.

\begin{enumerate}
\item Anstatt den Kerntensor zu modifizieren, können wir bei der Matrix-Matrix Multiplikation nur die Diagonalelemente für die Multiplikation in Erwägung ziehen. Damit sparen wir $O(N^4)$ Operationen.

\item Man kann den Kerntensor in $K_2$ vor dem Ausrechnen des Kronecker-Produkts verwerten. Selbst wenn wir dies erreichen, können wir die Komplexität von Option 1 nicht übertreffen, denn die Berechnung von $K_3$ würde dies nicht zulassen.

\item Eine letzte Alternative ist die trunkierte HOSVD.

\end{enumerate}

Wenn wir es schaffen dies effizient zu berechnen und sogar eine trunkierte HOSVD wählen, ist das Ergebnis nicht zielführend. Durch die trunkierte HOSVD geben wir wertvolle Informationen auf, bei der Entfaltung der Tensoren treten Schwierigkeiten auf und die Komplexität ist nicht annähernd so gut wie mit der Nutzung der Tensorstruktur.
Bei der HOSVD ist die alleinige Berechnung der Zerlegung mit einem hohen Aufwand verbunden. Das $n-mode$ Produkt liegt in $O(N^5)$. Es ist außerdem nicht möglich, diesen zu umgehen oder zu optimieren, da wir in unserem Fall voll besetzte Tensoren haben. 

Aus diesen Gründen würde ich diesen Ansatz nicht empfehlen.

