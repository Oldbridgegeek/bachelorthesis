In Kapitel 3 haben wir uns zwei Möglichkeiten angeschaut die Pseudoinversen herzuleiten. In beiden Möglichkeiten fanden wir das Kronecker-Produkt.

Das heißt um dies effizient zu implementieren, sollten wir uns Gedanken dadrüber machen, wie wir diese Struktur ausnutzen können.

\subsection{Effizientes Matrix-Vektor Produkt }
In \cite{Teachlet} wird eine Strategie vorgestellt ein Matrix-Vektor Produkt mit Kronecker Produkt Matrizen $z=(\mathcal{B} \otimes \mathcal{A})y$ effektiv zu berechnen.

Sei $\mathcal{A} \in \mathbb{R}^{m \times n}$ und $\mathcal{B} \in \mathbb{R}^{p \times q}$. Das Kronecker Produkt dieser Matrizen kann man schreiben als
\begin{equation*}
\mathcal{B} \times \mathcal{A} =
\begin{pmatrix}
b_{11}\mathcal{A} & \dots  & b_{1q}\mathcal{A} \\
\vdots & \ddots & \vdots \\
b_{p1}\mathcal{A} & \dots & b_{pq}\mathcal{A} \\
\end{pmatrix}.
\end{equation*}

Wir sehen sich wiederholende Strukturen von $\mathcal{A}$. Genau diese wollen wir uns zu Nutze machen. Nehmen wir an $y$ sei geordnet in der Indexierung.
\begin{equation*}
y = (y_1,y_2,\dots,y_n,\dots,\dots,y_{(q-1)n+1},y_{(q-1)n+2},\dots,y_{qn})^T
\end{equation*}

Wir denken uns nun die Faktoren $b_{ij}$ die mit $\mathcal{A}$ multipliziert werden erstmal weg. Definiere $y^{(1)}=(y_1,y_2,\dots,y_n)^T$.

\begin{equation*}
w^{(1)}=
\begin{pmatrix}
w_1 \\ \vdots \\ w_m 
\end{pmatrix}
=
\begin{pmatrix}
a_{11} & \dots & a_{1n} \\
\vdots & \dots & \vdots \\
a_{m1} & \dots & a_{mn} \\
\end{pmatrix}
\begin{pmatrix}
y_1 \\ \vdots \\ y_n
\end{pmatrix}
= \mathcal{A}y^{(1)}
\end{equation*}

Auf ähnliche Weise können wir uns $y^{(2)}=(y_{n+1},\dots,y_{2n})^{T}$ definieren und dann
\begin{equation*}
w^{(2)} = \mathcal{A}y^{(2)}
\end{equation*}

Wir führen dies so weiter und erhalten

\begin{equation*}
w=( (w^{(1)})^T , \dots, (w^{(q)})^T)^T \in \mathbb{R}^{mq}
\end{equation*}

Nun müssen wir die Informationen der Matrix B noch mit reinbringen. Dazu berechnen wir wie in \cite{Teachlet} vorgeschlagen $z_i$ mit

\begin{equation*}
z^{(k)}_j = \sum_{i=1}^{q} b_{ki} w_j^{(i)}
\end{equation*}

Der komplette Algorithmus ist nachfolgend dargestellt.
\begin{mdframed}[backgroundcolor=blue!3] 
\begin{algorithmic}
\For {i=1 < q }
	\For {j= 1 < m }
			\State $w^{(i)}_{j} = y^{(i)}_1 a_{j1} + \dots + y^{(i)}_n a_{jn}$
	\EndFor
\EndFor
\For {k=1 < n }
	\For {j= 1 < p }
			\State $z^{(k)}_j := w^{(1)}_j b_{k1} + \dots + w^{(q)}_j b_{kq}$
	\EndFor
\EndFor

\end{algorithmic}
\end{mdframed}


Nehmen wir für die triviale Berechnung der Komplexität an $n=m=p=q$.
Mit Hilfe dieses Algorithmus haben wir die Komplexität des Auswertens der Gleichung von $2m^4$ Operationen auf $4m^3$ Operationen reduziert.

Dieser Algorithmus ist optimal um das effektive Berechnen von
\begin{equation*}
Mu=[\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T]^+ \otimes [\mathcal{W}_N^{1D} (\mathcal{N}_{1D})^T]^+ u
\end{equation*}

Doch für den zweiten Ansatz über die HOSVD,  werden wir eine erweiterte Form des Algorithmus brauchen, da wir mehrere Kronecker Produkte bekommen. Außerdem kann der folgende Algorithmus dann auch für dreidimensionale Ansatzfunktionen bei dem ersten Ansatz verwendent werden.


\subsubsection{Erweiterung}
Wir wollen den Algorithmus erweitern für die Berechnung von $z=(\mathcal{C} \otimes \mathcal{B} \otimes \mathcal{A})v$ mit $\mathcal{A} \in \mathbb{R}^{N \times N}$, $\mathcal{B} \in \mathbb{R}^{N \times N}$, $\mathcal{C} \in \mathbb{R}^{N \times N}$ und $v \in \mathbb{R}^{N^3}$.


\begin{equation} z=
\begin{pmatrix}
c_{11} b_{11} \mathcal{A} & \dots  & c_{11} b_{1N} \mathcal{A} & \dots & \dots & c_{1N}b_{11}\mathcal{A} & \dots & c_{1N}b_{1N}\mathcal{A}  \\

\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{11} b_{N 1} \mathcal{A} & \dots  & c_{11} b_{N N} \mathcal{A} & \dots & \dots & c_{1 N}b_{N 1}\mathcal{A} & \dots & c_{1 N}b_{N N}\mathcal{A}  \\
c_{21} b_{11} \mathcal{A} & \dots  & c_{21} b_{1N} \mathcal{A} & \dots & \dots & c_{2N}b_{11}\mathcal{A} & \dots & c_{2 N}b_{1 N}\mathcal{A}  \\
\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{N 1} b_{N 1} \mathcal{A} & \dots  & c_{N 1} b_{N N} \mathcal{A} & \dots & \dots & c_{N N}b_{N 1}\mathcal{A} & \dots & c_{N N}b_{N N}\mathcal{A}  \\
\end{pmatrix} \, \, v
\end{equation}

Wir sehen hier sich zwei wiederholende Strukturen, die wir ausnutzen wollen um Operationen zu sparen.

\begin{equation} \label{eq:matrix} z=
\begin{pmatrix}
c_{11} \textcolor{green}{b}_{11} \textcolor{red}{\mathcal{A}} & \dots  & c_{11} \textcolor{green}{b}_{1N} \textcolor{red}{\mathcal{A}} & \dots & \dots & c_{1N}\textcolor{green}{b}_{11}\textcolor{red}{\mathcal{A}} & \dots & c_{1N}\textcolor{green}{b}_{1N}\textcolor{red}{\mathcal{A}}  \\

\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{11} \textcolor{green}{b}_{N 1} \textcolor{red}{\mathcal{A}} & \dots  & c_{11} \textcolor{green}{b}_{N N} \textcolor{red}{\mathcal{A}} & \dots & \dots & c_{1 N}\textcolor{green}{b}_{N 1}\textcolor{red}{\mathcal{A}} & \dots & c_{1 N}\textcolor{green}{b}_{N N}\textcolor{red}{\mathcal{A}}  \\
c_{21} \textcolor{green}{b}_{11} \textcolor{red}{\mathcal{A}} & \dots  & c_{21} \textcolor{green}{b}_{1N} \textcolor{red}{\mathcal{A}} & \dots & \dots & c_{2N}\textcolor{green}{b}_{11}\textcolor{red}{\mathcal{A}} & \dots & c_{2 N}\textcolor{green}{b}_{1 N}\textcolor{red}{\mathcal{A}}  \\
\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{N 1} \textcolor{green}{b}_{N 1} \textcolor{red}{\mathcal{A}} & \dots  & c_{N 1} \textcolor{green}{b}_{N N} \textcolor{red}{\mathcal{A}} & \dots & \dots & c_{N N}\textcolor{green}{b}_{N 1}\textcolor{red}{\mathcal{A}} & \dots & c_{N N}\textcolor{green}{b}_{N N}\textcolor{red}{\mathcal{A}}  \\
\end{pmatrix} \, \, v
\end{equation}

Den Vektor $v$ können wir zu einem Tensor $\mathcal{V} \in \mathbb{R}^{N \times N \times N} $ umdefinieren, damit dieser handlicher für unsere Anwendung wird. Der erste Index repräsentiert in welchen Spalteneintrag von $C$, der Zweite in welchen Spalteneintrag von $B$ und der Dritte in welchen Spalteneintrag von $A$ wir uns befinden. Dies kann man wieder als eine Index-Transformation ansehen, die bestimmte Einträge von $v$ eindeutig auf Tensorelemente abbildet. 
Wir schauen uns erstmal die einzelnen Einträge von $z$ an. 

\begin{equation*} \label{eq:zold}
\begin{aligned}
z_{1} = \mathcal{V}(1,1,1) c_{11} \textcolor{green}{b_{11}} \textcolor{red}{a_{11}}+ \dots +  \mathcal{V}(1,1,N) c_{11} \textcolor{green}{b_{11}} \textcolor{red}{a_{1N}} + \dots  +  \mathcal{V}(1,N,1)c_{11} 
\textcolor{green}{b_{1N}} \textcolor{red}{a_{11}} \\ + \dots +  \mathcal{V}(N,1,1) c_{1N} \textcolor{green}{b_{11}} \textcolor{red}{a_{11}} + \dots +  \mathcal{V}(N,N,N) c_{1N} \textcolor{green}{b_{1N}} \textcolor{red}{a_{1N}}
\end{aligned}
\end{equation*}
Definiere $w_{\textcolor{red}{1}}(i,j) :=\mathcal{V}(i,j,1) a_{\textcolor{red}{1}1}+\dots+\mathcal{V}(i,j,N) a_{\textcolor{red}{1}N}$. Dann erhalten wir
\begin{equation*}
\begin{aligned}
z_{1}= w_1(1,1) c_{11} b_{11} + \dots +   w_1(1,N) c_{11} b_{1N} + \dots + w_1(n,1) c_{1N} b_{11}  + \dots +  w_1(N,N) c_{1N} b_{1N}.
\end{aligned}
\end{equation*}

Damit haben wir uns die sich wiederholende Struktur von der Matrix $\textcolor{red}{\mathcal{A}}$ zu nutze gemacht. Im nächsten Schritt machen wir uns die sich wiederholende Struktur von $\textcolor{green}{b_{ij}}$ zu nutze.
Wir definieren hierfür $\mathcal{W}_{\textcolor{red}{1},k} (i):= w_k(i,1) b_{\textcolor{red}{1}1} + \dots + w_k(i,N) b_{\textcolor{red}{1}N}$. Damit erhalten wir
\begin{equation} \label{eq:znew}
\begin{aligned}
z_{1}= \mathcal{W}_{1,1}(1) c_{11}  + \dots +  \mathcal{W}_{1,1}(N) c_{1N}.
\end{aligned}
\end{equation}

Wir formen $z$ genau so wie wir das auch für $v$ gemacht haben und erhalten dafür den Tensor $\mathcal{Z}$. Damit erhalten wir für allgemeines $z_{i}$ folgende Formel
\begin{equation}
\mathcal{Z}(i,j,k) = \mathcal{W}_{j,k}(1) c_{i1}  + \dots +  \mathcal{W}_{j,k}(N) c_{iN} \, ,
\end{equation}
wobei j und k den Zeilen jeweils in den Matrizen $\mathcal{B}$ und $\mathcal{C}$ entsprechen. 
\newpage
Der komplette Algorithmus ist nachfolgend dargestellt.
\begin{mdframed}[backgroundcolor=blue!3] 
\begin{algorithmic}
\For {k=1 < N }
	\For {i= 1 < N }
		\For {j= 1 < N }
			\State $w_{k}(i,j) = \mathcal{V}(i,j,1)a_{k1} + \dots + \mathcal{V}(i,j,N)a_{kN}$
		\EndFor
	\EndFor
\EndFor
\For {k=1 < N }
	\For {i= 1 < N }
		\For {j= 1 < N }
			\State $\mathcal{W}_{i,j} (k):= w_j(k,1) b_{i1} + \dots + w_j(k,N) b_{iN}$
		\EndFor
	\EndFor
\EndFor
\For {k=1 < N }
	\For {i= 1 < N }
		\For {j= 1 < N }
			\State $\mathcal{Z}(i,j,k) = \mathcal{W}_{j,k}(1) c_{i1}  + \dots +  \mathcal{W}_{j,k}(N) c_{iN}$ 
		\EndFor
	\EndFor
\EndFor
\end{algorithmic}
\end{mdframed}

 
Wir haben in (\ref{eq:matrix}) eine Matrix-Vektor Multiplikation von einer Matrix der Größe $N^3 \times N^3$. Dementsprechend hätten wir $N^{6}$ Multiplikationen und $N^6$ Additionen. Dies entspricht $2N^6$ elementare Operationen. Die Komplexität des vorgeschlagenen Algorithmuses reduziert die Operationen auf $3N^{4}$ Multiplikationen und genau so viele Additionen. Somit haben wir insgesamt $6N^4$ Operationen. Das ist eine beträchltiche Reduktion, vor allem für großes $N$.
\newline

\newpage
\subsection{Anwendung}

Wie können wir uns nun diese Algorithmen zu nutze machen für die in Kapitel 3 besprochenen Strategien?

\subsubsection{Summenfaktorisierung}

Aus der Tensorsturktur aus Kapitel 3 für die lokale Massematrix hergeleitet. Nun schauen wir uns das Matrix-Vektor Produkt mit den Pseudoinversen als Matrix und einen beliebigen Vektor $u$. 

\textbf{Masse Matrix} 
\begin{equation*}
M^{+}u =  [(\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T)^+ \otimes (\mathcal{W}_N^{1D} (\mathcal{N}_{1D})^T)^+]u.
\end{equation*}



Wie komplex ist es die Pseudoinversen der Matrizen $(\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T)$  zu berechnen?

Wir können als Ansatz eine Singulärwertzerlegung wählen. Aus der Singulärwertzerlegung müssen wir noch die Pseudoinverse erst herleiten.
Allgemein gilt
\begin{equation*}
M\,=\,U\Sigma V^{T}
\end{equation*}
für eine eine $m \times n$-Matrix $M$ mit Rang $r$, wobei
$U$ eine orthogonale $m\times m$-Matrix, $V^{T}$ die Transponierte einer orthogonalen $n\times n$-Matrix $V$ und
$\Sigma$  eine reelle  $m\times n$-Diagonalmatrix ist.

Die Pseudoinverse daraus hergeleitet ergibt
\begin{equation*}
M^{+}=V\Sigma ^{+}U^{T}.
\end{equation*}

Die Komplexität für die Herleitung der Pseudoinversen aus der Singulärwertzerlegung ist vernachlässigbar. Für die Berechnung von $\Sigma^+$ haben wir $r$ Operationen, da in $\Sigma$ in der Diagonalen $r$ Einträge stehen, die wir einfach nur invertieren müssen. Die Berechnung von $U^{T}$ ist vernachlässigbar, da man bei der Berechnung der Singulärwertzerlegung direkt $U^{T}$ speichern kann anstatt $U$.
Also bekommen wir mit der Singulärwertzerlegung einer Matrix $M \in \mathbb{R}^{m \times n}$ die Pseudoinverse mit einer Komplexität von $O(min(mn^2,m^2 n))$ \cite[2]{SVD}. Man könnte natürlich hier auch approximative Verfahren wählen und versuchen an Operationen zu sparen. Ich verweise hier auf \cite{SVD} für die nähere Betrachtung von schnelleren Singulärwertzerlegungen.


Wir können stattdessen mit dem Gauß Algorithmus oder mit der Neumann Reihe arbeiten, wenn $m=n$. Gauß Algorithmus gibt uns eine Komplexität von $O(n^3)$. 
Neumann Reihe macht keinen Sinn, da die Komplexität mindestens genau so hoch ist wie bei Gauß.

Gauß Algorithmus macht natürlich nur einen Sinn, wenn $(\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T)$ invertierbar ist.

\begin{Bemerkung} (Invertierbarkeit) \\ \label{bem:unit}
Es seien $\mathcal{W}_N^{1D} \in \mathbb{R}^{n \times n}$ und $(\mathcal{N}^{1D})^T \in \mathbb{R}^{n \times n}$ Matrizen, die wie gewohnt definiert sind. Dann gilt, dass $(\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T) \in \mathbb{R}^{n \times n}$ invertierbar ist.

\begin{proof}
Man kann durch einfache Basistransformation erreichen, \\ dass $(\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T) \in \mathbb{R}^{n \times n}$  die Einheitsmatrix ist. Es seien $x=\{x_1,\dots,x_n\}$ die Stützstellen und $w=\{w_1,\dots,w_n\}$ die Gewichte der Quadratur.

Wir wählen 
\begin{equation*}
\varphi^{1D}_i (x_k) = \dfrac{1}{ \sqrt{w_i} } l_i (x_k) = 
\begin{cases}
\dfrac{1}{ \sqrt{w_i} } \, \text{ , wenn } i=k  \\
0  \, \, \, \, \, \, \, \, \, \, \, \text{ , sonst }
\end{cases}
\end{equation*}

Die Funktion $l_i(x_k)$ bezeichnet das $i-te$ Lagrange Polynom ist.
Es folgt 
\begin{equation*}
\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T = \mathcal{N}^{1D} \mathcal{W}^{1D} (\mathcal{N}^{1D})^T = I_n
\end{equation*}

Da $\mathcal{N}$ eine Diagonalmatrix ist, mit den Diagonaleinträgen $\dfrac{1}{ \sqrt{w_i} }$, gilt $\mathcal{N}=(\mathcal{N})^T$. Weiterhin ist $\mathcal{W}$ auch eine Diagonalmatrix mit den Einträgen $w_i$ in der Diagonalen. In der Ergebnismatrix steht dann in der Diagonalen 
\begin{equation*}
(\dfrac{1}{ \sqrt{w_i} })^2  w_i = 1 \, .
\end{equation*}
 
\end{proof}
\end{Bemerkung}

Insgesamt erhalten wir für die Berechnung der Inversen bzw. der Pseudoinversen folgende Komplexität:
\begin{itemize}
\item Um das Matrix-Vektor Produkt effizient zu berechnen nutzen wir einfach den Algorithmus für die Berechnung von $z=(\mathcal{B} \otimes \mathcal{A})y$ aus Kapitel 4.1. und erhalten eine Komplexität von $4n^3$.

\item Matrizenmultiplikation ist kubisch.

\item Berechnung der Pseudoinversen/Inversen liegt in $O(n^3)$

\end{itemize}

Insgesamt haben wir eine Komplexität von $O(4n^3)+O(n^3)+O(n^3)=O(6n^3)$.
Wie wir noch weiter an Komplexität sparen können und wo wir anknüpfen können um effizienter zu werden, wird in Kapitel 5 diskutiert.

Wir kommen zu dem Problem mit der Laplace Bilinearform.  \\
\textbf{Laplace Bilinearform}
\begin{equation*}
V^{+}u = \big{(} [(\widehat{\mathcal{W}}_N^{1D} (\widehat{\mathcal{N}}^{1D})^T] \otimes [\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T]  +  [\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T] \otimes [\widehat{\mathcal{W}}_N^{1D} (\widehat{\mathcal{N}}^{1D})^T]  \big{)}^+ u
\end{equation*}

Wir wollen dies weiter vereinfachen. Definiere $A=[(\widehat{\mathcal{W}}_N^{1D} (\widehat{\mathcal{N}}^{1D})^T]$. Wähle Basis so, dass $\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T  = I_n$. Dies geht, wie in Beweis für Bemerkung (\ref{bem:unit}).

Wir erhalten folgende vereinfachte Form.
\begin{equation*} \label{eq:easy}
Vu =[(A \otimes I_) + (I_n \otimes A)]u.
\end{equation*}

Mit der in Kapital 4.1 besprochenen Strategie, können wir den Vektor $u$ in eine Matrix umformen, sodass wir (\ref{eq:easy}) äquivalent umformen können zu
\begin{equation*} \label{eq:easy}
V=V^{+}U =[(A \otimes I_n) + (I_n \otimes A)]U,
\end{equation*}
wobei $U$  und $V$ definiert sind durch $U_{ij}=u(q_i,q_j)$ und $V_{ij}=v(q_i,q_j)$

Dies können wir weiter vereinfachen.
\begin{equation*} \label{eq:easy}
V=(A \otimes I_n)U + (I_n \otimes A)U = AUI_n + I_n U A^T = AU + UA^T
\end{equation*}

Dies ist gerade die Lyapunow Gleichung aus der Stabilitätstheorie. Zu der Lösung der Lyapunow Gleichung gibt es zahlreiche Untersuchungen. Die Behandlung dieser Gleichung würde den Rahmen dieser Bachelorarbeit sprengen. Ich empfehle als Literatur für diese Untersuchungen \cite{Lyapunov}.

Die Lösung der Gleichung ist gegeben durch
\begin{equation*}
U = \int\limits_{0}^{\infty} e^{A \tau} (-V) e^{A^T \tau} d\tau.
\end{equation*}

Die naive Berechnung der Lösung dieser Gleichung erfolgt mit einer Komplexität von $O(n^6)$. Der Bartels-Stewart Algorithmus \cite{Bartels} liefert uns eine Komplexität von $O(n^3)$. 

\subsection{Singulärwertzerlegung höherer Ordnung}
Nun haben wir uns mit Hilfe der Tucker Dekomposition eine Herleitung für die Pseudoinverse erarbeitet. Jetzt geht es um die effiziente Berechnung dieser Formel. Dazu wollen wir uns die Strategie zu nutze machen, die wir in Kapitel 4.1.1. hergeleitet haben für die effektive Berechnung von zwei Kronecker Produkten mit einem Vektor.
Es sei $\mathscr{X} \in \mathbb{R}^{I_1 \times \dots \times I_{4}}$ ein Tensor. Der Tensor $\mathscr{X}$ könnte der Massetensor sein oder der Laplace Bilinearform Tensor. Da die Massematrix und die Elementsteifigkeitsmatrix quadratisch sind, gilt $N:= I_1 = \dots = I_4$.
Die Formel für die Pseudoinverse lautet

\begin{equation} \label{eq:pinv}
\mathscr{X}^{+} = \mathscr{G}^{+} \times_{n=1}^{4} U^{ (n) ^{T} }
\end{equation}

Wobei $\mathscr{G} \in \mathbb{R}^{N \times N \times N \times N}$ und $U^{(n)} \in \mathbb{R}^{N \times N}$.

Wie aufwändig ist es die HOSVD zu berechnen?
In Bemerkung (\ref{hosvd}) haben wir den Algorithmus für die Berechnung der HOSVD. 
\begin{enumerate}
\item Für den ersten Schritt des Algorithmuses müssen wir alle $mode-k$ Entfaltungen berechnen. Dies impliziert eine Ordnung jedes Element des Tensors in ein Element der Ergebnismatrix. Damit haben wir genau so viele Zuweisungen, wie Anzahl der Elemente des Tensors. In unserem Fall wären das $4N^4$ Operationen für alle modes. 

\item Die Singulärwertzerlegung für die Entfaltungen gilt es im zweiten Schritt zu berechnen. Für jede SVD haben wir die Komplexitätsklasse $O(N^4)$. Insgesamt $O(4N^4)$.

\item Im dritten Schritt wollen wir den Kerntensor berechnen. Dazu müssen wir n-mode Produkte berechnen. Ein n-mode Produkt hat  $N^5$ Additionen und genau so viele Multiplikationen. Davon müssen wir vier berechnen. Insgesamt haben wir eine Komplexität von $8N^5$ Rechenoperationen.

\end{enumerate}

Insgesamt ergibt sich eine Komplexität von $O(8N^4  + 8N^5)$. 
Die Berechnung der orthogonal Matrizen für die HOSVD zeigt, dass alle Matrizen gleich sind. Dementsprechend müssen wir im zweiten Schritt die SVD nur einmal berechnen. Damit reduziert sich die Komplexität zu $O(5N^4  + 8N^5)$. \\

Im nächsten Schritt betrachten wir das Matrix-Vektor Produkt mit einem beliebigen Vektor $u \in \mathbb{R}^{N^3}$ und überlegen wie wir uns die Strukturen dort zu nutze machen.

\subsubsection{Naiver Ansatz}
Wir schauen uns zu erst einen naiven Ansatz an. Dafür benötigen wir jedoch noch ein geeigntes Tensor-Vektor Produkt. 
Dazu schauen wir uns das Matrix-Vektor Produkt an. Sei $A \in \mathbb{R}^{m \times n}$ und $x \in \mathbb{R}^{n}$. Dann ist $y=Ax \in \mathbb{R}^{m}$ definiert durch
\begin{equation*}
y_i = \sum\limits_{j=1}^{n} a_{ij} x_j
\end{equation*}
Durch die Index-Transformation $p(\cdot)$ können wir uns das Ganze nun für Tensoren herleiten.

\begin{Definition} (Tensor-Vektor Produkt) \\
Es sei $\mathcal{A} \in \mathbb{R}^{N \times N \times N \times N}$ und $U \in \mathbb{R}^{N \times N}$.
Dann ist $Y=tvp(\mathcal{A},U)$ gegeben durch

\begin{equation*}
Y_{i_1 \, i_2} = \sum\limits_{j_1=1}^{N} \sum\limits_{j_2=1}^{N} \mathcal{A}_{i_1 i_2 j_1 j_2} U_{j_1 j_2}
\end{equation*}

\end{Definition}

Nun können wir uns den naiven Ansatz anschauen, der wie folgt gegeben ist
\begin{equation} \label{eq:pinv}
tvp(\mathscr{X}^{+},U) = tvp(\mathscr{G}^{+} \times_{n=1}^{4} U^{ (n) ^{T} },U) ,
\end{equation} 
mit $U \in \mathbb{R}^{N \times N}$. Das ist das Matrix-Vektor Produkt in Tensorform. 

Die Komplexität für die Berechnung ist gegeben durch $N^4$. Denn wir haben für die Berechnung eines einzelnen Elementes $Y_{i_1 i_2}$ zwei Schleifen die bis $N$ durchlaufen. Insgesamt haben wir $N^2$ Elemente. Dies ergibt $N^4$. 
Mit der Berechnung der HOSVD ergibt dieser Ansatz eine Komplexität von $O(6N^4 + 8N^5)$. \\

\subsubsection{Entfaltete HOSVD}
Wir wollen nun einen anderen Ansatz wählen und es auszunutzen, dass wir die HOSVD in entfalteter Form betrachten können.
Man kann (\ref{eq:pinv}) wie in (\ref{eq:tensortensor}) äquivalent umformen zu

\begin{equation}
\begin{aligned}
\mathscr{X}^{+}_{(n)}  &= U^{ (n) ^{T} }  \mathscr{G}^{+}_{(n)} ( U^{ (4) ^{T} } \otimes \dots \otimes U^{ (n+1) ^{T} } \otimes U^{ (n-1) ^{T} } \otimes \dots \otimes U^{ (1) ^{T} })^{T} \\ \iff
\mathscr{X}^{+}_{(n)} &= U^{ (n)  }  \mathscr{G}^{+}_{(n)} ( U^{ (4)  } \otimes \dots \otimes U^{ (n+1) } \otimes U^{ (n-1) } \otimes \dots \otimes U^{ (1) })
\end{aligned}
\end{equation}

Die Äquivalenz folgt mit Lemma (\ref{lemma:transpose}).

\begin{equation} \label{eq:pinvv}
\begin{aligned}
\mathscr{X}^{+}_{(n)}v&= U^{ (n)  }  \mathscr{G}^{+}_{(n)} ( U^{ (4)  } \otimes \dots \otimes U^{ (n+1) } \otimes U^{ (n-1) } \otimes \dots \otimes U^{ (1) }) u
\end{aligned}
\end{equation}

Wir führen die Variablen $N_i$ ein mit $N_i \neq n$. Damit können wir (\ref{eq:pinv}) reduzieren auf

\begin{equation} \label{eq:pinvcase}
\begin{aligned}
\mathscr{X}^{+}_{(n)} u&= \underbrace{\underbrace{U^{ (n) }  \mathscr{G}^{+}_{(n)}}_{K_1} \underbrace{( U^{ (N_{1})  } \otimes U^{ (N_{2})}  \otimes U^{ (N_{3}) }) u}_{K_2}}_{K_3} \,.
\end{aligned}
\end{equation}
 

Wir erkennen, dass wir unsere Methodik nutzen können, die wir entwickelt haben um $K_2$ effizient zu berechnen. Dies ist möglich mit einer Komplexität von $O(6N^4)$.

Wie schaffen wir es den vollbesetzten Kerntensor zu invertieren bzw. die Pseudoinverse herzuleiten? Wie wir wissen, müssen wir dazu erstmal Zahlen deren Betrag kleiner ist als $\epsilon > 0$ auf Null setzen und die restlichen Elemente einzeln invertieren. 
Dazu müssen wir, da der Kerntensor vollbesetzt ist, durch alle Elemente des Tensors durch-iterieren. Das heißt, wir haben eine Komplexität von $O(N^4)$, da unser Tensor Ordnung 4 hat.
Es folgt eine letzte Matrix-Matrix Multiplikation $U^{ (n) ^{T} }  \mathscr{G}^{+}_{(n)}$. Wie groß sind diese Matrizen? Die Matrix $ \mathscr{G}^{+}_{(n)}$ hat die Größe $N \times N^3$. Die Matrix  $U^{ (n) ^{T} }$ hat die Größe $N \times N$.  Wir haben bei der Multiplikation dieser beiden Matrizen $N^6$ Multiplikationen und genau so viele Additionen. Dann können wir die Komplexitätsklassen durch $O(N^6)$ angeben. Die Anzahl der Operationen steigt mit raketengeschwindigkeit durch diese Matrix-Matrix Multiplikation. Doch dank der Struktur des Kerntensors reduziert sich die Komplexität auf $O(N^4)$. Das liegt daran, dass der Tensor $ \mathscr{G}^{+}$ super-diagonal ist. Damit müssen wir also pro Element der Ergebnismatrix nicht die Spalte mal Zeile rechnen, sondern einfach nur das Diagonalelement mal ein Element der Matrix. 

Nun gilt es das Ergebnis von $K_1$ und das Ergebnis von $K_2$ zusammen zu rechnen durch ein Matrix-Matrix Produkt. Die Matrix $K_1$ hat die Größe $N \times N^3$ und $K_2$ ist $N^3 \times N^3$ groß. 
Die Anzahl der Operationen für die Berechnung von $K_3$ ist $N^{10}$.

Insgesamt haben wir
\begin{itemize}
\item $O(N^4)$ für die Berechnung von $K_1$,
\item $O(6N^4)$ für die Berechnung von $K_2$.
\item $O(N^{10})$ für die Berechnung von $K_3$.
\end{itemize}

Zusammengefasst $O(7N^4+N^{10})$. \\ Dies ist aber nicht annähernd so gut wie Option 1. Man bemerke, dass wir die Komplexität von der Berechnung der HOSVD noch nicht dazu addiert haben. Wie können wir unsere effizienz weiter steigern?

Wir können uns eine trunkierte Singulärwertzerlegung höherer Ordnung  wagen oder eine andere Alternative finden. Folgende Möglichkeiten zeigen sich

\begin{enumerate}
\item Anstatt den Kerntensor zu modifizieren, können wir bei der Matrix-Matrix Multiplikation einfach nur die Diagonalelemente für die Multiplikation in Erwägung ziehen. Damit sparen wir uns $O(N^4)$ Operationen.

\item Was wir tun könnten ist den Kerntensor in $K_2$ reinmultiplizieren bevor wir das Kronecker Produkt ausrechnen (siehe dazu die Bemerkung (\ref{bem:diagonal})). Selbst wenn wir das hinkriegen, würden wir es nicht hinbekommen die Komplexität von Option 1 zu übertreffen, denn die Berechnung von $K_3$ würde uns immer noch einen gewaltigen Strich durch die Rechnung machen.

\item Eine letzte Alternative ist die trunkierte HOSVD.

\end{enumerate}

Das Problem ist, dass unser entfalteter Kerntensor keine Diagonalmatrix ist. Der Kerntensor ist an sich zwar super-diagonal, bloß hängt es von der Entfaltung ab, ob der entfaltete Tensor eine Diagonalmatrix ist. 
Der entfaltete Kerntensor ist genau dann eine Diagonalmatrix, wenn wir die natürliche Entfaltung wählen. Mit dieser Entfaltung ist die Entfaltung gemeint mit Hilfe der Index Transformation $p(\cdot)$.

Selbst wenn wir es schaffen dies effizient zu berechnen und sogar noch eine trunkierte HOSVD wählen. Ist das Ergebnis das was wir wollen? Durch die trunkierte HOSVD geben wir wertvolle Informationen auf, bei der Entfaltung der Tensoren treten Schwierigkeiten auf und die Komplexität ist nicht annähernd so gut wie mit der Ausnutzung der Tensorstruktur.

