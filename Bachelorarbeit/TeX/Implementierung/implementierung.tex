In Kapitel 3 haben wir uns zwei Möglichkeiten angeschaut, das Matrix-Vektor Produt zu berechnen mit der Pseudoinversen. Beide Alternativen bargen eine Tensorprodukt Struktur, in Form des Matrix-Kronecker Produkt.

Das heißt um dies effizient zu implementieren sollten wir uns Gedanken dadrüber machen wie wir diese Struktur ausnutzen können.

Im Exa-dg-teachlet wird eine Strategie vorgestellt ein Matrix-vektor Produkt mit Kronecker Matrizen also $z=(\mathcal{B} \otimes \mathcal{A})y$ berechnet werden kann.

Sei $\mathcal{A} \in \mathbb{R}^{m \times n}$ und $\mathcal{B} \in \mathbb{R}^{p \times q}$. Das Kronecker Produkt dieser Matrizen kann man schreiben als, 
\begin{equation*}
\mathcal{B} \times \mathcal{A} =
\begin{pmatrix}
b_{11}\mathcal{A} & \dots  & b_{1q}\mathcal{A} \\
\vdots & \ddots & \vdots \\
b_{p1}\mathcal{A} & \dots & b_{pq}\mathcal{A} \\
\end{pmatrix}
\end{equation*}

Nun wir sehen also die sich wiederholende Struktur von $\mathcal{A}$. Genau diese wollen wir uns nun zu nutze machen. Nehmen wir an y sei geordnet in der Indexierung.
\begin{equation*}
y = (y_1,y_2,\dots,y_n,\dots,\dots,y_{(q-1)n+1},y_{(q-1)n+2},\dots,y_{qn})^T
\end{equation*}

Wir denken uns nun die Faktoren $b_{ij}$ die mit $\mathcal{A}$ multipliziert werden erstmal weg. Definiere $y^{(1)}=(y_1,y_2,\dots,y_n)^T$.

\begin{equation*}
w^{(1)}=
\begin{pmatrix}
w_1 \\ \vdots \\ w_m 
\end{pmatrix}
=
\begin{pmatrix}
a_{11} & \dots & a_{1n} \\
\vdots & \dots & \vdots \\
a_{m1} & \dots & a_{mn} \\
\end{pmatrix}
\begin{pmatrix}
y_1 \\ \vdots \\ y_n
\end{pmatrix}
= \mathcal{A}y^{(1)}
\end{equation*}

Auf ähnliche Weise können wir uns $y^{(2)}=(y_{n+1},\dots,y_{2n})^{T}$ definieren und dann
\begin{equation*}
w^{(2)} = \mathcal{A}y^{(2)}
\end{equation*}

Wir führen dies so weiter und erhalten

\begin{equation*}
w=( (w^{(1)})^T , \dots, (w^{(q)})^T) \in \mathbb{R}^{mq}
\end{equation*}

Nun müssen wir die Informationen der Matrix B noch mit reinbringen. Dazu berechnen wir wie im Teachlet auch vorgeschlagen $z_i$ mit

\begin{equation*}
z_i = \sum_{i=1}^{q} b_{1i} w_m^{(i)}
\end{equation*}

Mit Hilfe diesen Algorithmus haben wir die Komplexität von $2m^4$ auf $4m^3$ reduziert.
Nun wollen wir dies erweitern auf $z=(\mathcal{C} \otimes \mathcal{B} \otimes \mathcal{A})v$.


\begin{equation} z:=
\begin{pmatrix}
c_{11} b_{11} A & \dots  & c_{11} b_{1n} A & \dots & \dots & c_{1n}b_{11}A & \dots & c_{1n}b_{1n}A  \\

\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{11} b_{n1} A & \dots  & c_{11} b_{nn} A & \dots & \dots & c_{1n}b_{n1}A & \dots & c_{1n}b_{nn}A  \\
c_{21} b_{n1} A & \dots  & c_{21} b_{nn} A & \dots & \dots & c_{2n}b_{n1}A & \dots & c_{2n}b_{nn}A  \\
\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{n1} b_{n1} A & \dots  & c_{n1} b_{nn} A & \dots & \dots & c_{nn}b_{n1}A & \dots & c_{nn}b_{nn}A  \\
\end{pmatrix} * v
\end{equation}

Wir sehen hier sich zwei wiederholende Strukturen die wir ausnutzen können um bei einem Matrix-Vektor Produkt operationen zu sparen.

\begin{equation} \label{eq:matrix} z=
\begin{pmatrix}
c_{11} \textcolor{green}{b_{11}} \textcolor{red}{A} & \dots  & c_{11} \textcolor{green}{b_{1n}} \textcolor{red}{A} & \dots & \dots & c_{1n}\textcolor{green}{b_{11}}\textcolor{red}{A} & \dots & c_{1n}\textcolor{green}{b_{1n}}\textcolor{red}{A}  \\

\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{11} \textcolor{green}{b_{n1}} \textcolor{red}{A} & \dots  & c_{11} \textcolor{green}{b_{nn}} \textcolor{red}{A} & \dots & \dots & c_{1n}\textcolor{green}{b_{n1}}\textcolor{red}{A} & \dots & c_{1n}\textcolor{green}{b_{nn}}\textcolor{red}{A}  \\
c_{21} \textcolor{green}{b_{n1}} \textcolor{red}{A} & \dots  & c_{21} \textcolor{green}{b_{nn}} \textcolor{red}{A} & \dots & \dots & c_{2n}\textcolor{green}{b_{n1}}\textcolor{red}{A} & \dots & c_{2n}\textcolor{green}{b_{nn}}\textcolor{red}{A}  \\
\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{n1} \textcolor{green}{b_{n1}} \textcolor{red}{A} & \dots  & c_{n1} \textcolor{green}{b_{nn}} \textcolor{red}{A} & \dots & \dots & c_{nn}\textcolor{green}{b_{n1}}\textcolor{red}{A} & \dots & c_{nn}\textcolor{green}{b_{nn}}\textcolor{red}{A}  \\
\end{pmatrix} * v
\end{equation}

Nun wollen wir uns dies zu nutze machen. Wir schauen uns erstmal die einzelnen Einträge von z an und bekommen. Vorher definieren wir unser v um zu einem Tensor $\mathcal{V} \in \mathbb{R}^{I_1 \times I_2 \times I_3} $. Der erste Index repräsentiert in welcher Spalteneintrag von C wir uns befinden, der zweite in welchem Spalteneintrag von B und der Dritte in welchem Spalteneintrag von A. 
\begin{equation} \label{eq:zold}
\begin{aligned}
z_{1} = \mathcal{V}(1,1,1) c_{11} \textcolor{green}{b_{11}} \textcolor{red}{a_{11}}+ \dots +  \mathcal{V}(1,1,n) c_{11} \textcolor{green}{b_{11}} \textcolor{red}{a_{1n}} + \dots  +  \mathcal{V}(1,n,1)c_{11} 
\textcolor{green}{b_{1n}} \textcolor{red}{a_{11}} \\ + \dots +  \mathcal{V}(n,1,1) c_{1n} \textcolor{green}{b_{11}} \textcolor{red}{a_{11}} + \dots +  \mathcal{V}(n,n,n) c_{1n} \textcolor{green}{b_{1n}} \textcolor{red}{a_{1n}}
\end{aligned}
\end{equation}
Definiere $w_{\textcolor{red}{1}}(i,j) :=\mathcal{V}(i,j,1) a_{\textcolor{red}{1}1}+\dots+\mathcal{V}(i,j,n) a_{\textcolor{red}{1}n}$. Dann erhalten wir:
\begin{equation*}
\begin{aligned}
z_{1}= w_1(1,1) c_{11} b_{11} + \dots +   w_1(1,n) c_{11} b_{1n} + \dots + w_1(n,1) c_{1n} b_{11}  + \dots +  w_1(n,n) c_{1n} b_{1n} 
\end{aligned}
\end{equation*}

Damit haben wir uns die sich wiederholende Struktur von der Matrix \textcolor{red}{A} zu nutze gemacht. Im nächsten Schritt machen wir uns die sich wiederholende Struktur von $\textcolor{green}{b_{ij}}$ zu nutze.
Definiere hierfür $\textswab{W}_{\textcolor{red}{1},k} (i):= w_k(i,1) b_{\textcolor{red}{1}1} + \dots + w_k(i,n) b_{\textcolor{red}{1}n}$. Damit erhalten wir:

\begin{equation} \label{eq:znew}
\begin{aligned}
z_{1}= \textswab{W}_{1,1}(1) c_{11}  + \dots +  \textswab{W}_{1,1}(n) c_{1n} 
\end{aligned}
\end{equation}

Wir wollen nun $z$ genau so umformen wie wir das auch für v gemacht haben. Damit erhalten wir für allgemeines $z_{i}$ folgende Formel:
\begin{equation}
\mathcal{Z}(i,j,k) = \textswab{W}_{j,k}(1) c_{i1}  + \dots +  \textswab{W}_{j,k}(n) c_{in} 
\end{equation}
Wobei j und k den Zeilen jeweils in den Matrizen B und C entsprechen. 

Der komplette Algorithmus würde nun wie folgt aussehen:
\begin{framed}
\begin{algorithmic}
\For {k=1 < n }
	\For {i= 1 < n }
		\For {j= 1 < n }
			\State $w_{k}(i,j) = \mathcal{V}(i,j,1)a_{k1} + \dots + \mathcal{V}(i,j,n)a_{kn}$
		\EndFor
	\EndFor
\EndFor
\For {k=1 < n }
	\For {i= 1 < n }
		\For {j= 1 < n }
			\State $\textswab{W}_{i,j} (k):= w_k(i,1) b_{11} + \dots + w_k(i,n) b_{1n}$
		\EndFor
	\EndFor
\EndFor
\For {k=1 < n }
	\For {i= 1 < n }
		\For {j= 1 < n }
			\State $\mathcal{Z}(i,j,k) = \textswab{W}_{j,k}(1) c_{i1}  + \dots +  \textswab{W}_{j,k}(n) c_{in}$ 
		\EndFor
	\EndFor
\EndFor
\end{algorithmic}
\end{framed}

 
Wenn wir annehmen, dass die Matrizen $A,B,C \in \mathbb{R}^{n \times n}$. Dann haben wir bei \ref{eq:matrix} eine Matrix-Vektor Multiplikation von einer Matrix der Größe $n^{3} \times n^{3}$. Dementsprechend hätten wir $n^{6}$ Multiplikationen und $n^{6}$ Additionen. Die Komplexität des vorgeschlagenen Algorithmuses reduziert sich auf $3n^{3}$ Multiplikationen und genau so viele Additionen.
Ein enorme Reduktion, vor allem für großes n.

\textbf{Wie können wir uns nun diese Algorithmen zu nutze machen für die in Kapitel 3 besprochenen Strategien?} \\

\textbf{Summenfaktorisierung}

Rekapituliere die Formeln für die Masse Matrix und für die Laplace Bilinearform \\
\textbf{Masse Matrix} 
\begin{equation*}
M^{-1}*u = ((\mathcal{W} \mathcal{N}^{T}) \otimes (\mathcal{W} \mathcal{N}^{T}))^{-1}*u
= ((\mathcal{W} \mathcal{N}^{T})^{-1} \otimes (\mathcal{W} \mathcal{N}^{T})^{-1})*u
\end{equation*}

\textbf{Laplace Bilinearform}
\begin{equation*}
\mathcal{V}^{-1}*u =  -((\mathcal{W} \mathcal{N''}^{T})^{-1} \otimes (\mathcal{W} \mathcal{N}^{T})^{-1} - (\mathcal{W} \mathcal{N}^{T})^{-1} \otimes (\mathcal{W} \mathcal{N''}^{T})^{-1})*u
\end{equation*}

Um dies effizient zu berechnen nutzen wir einfach die Formeln aus dem Teachlet und erhalten eine Komplexität von:
Matrizenmultiplikation ist kubisch also $n^3$. Das Kroneckerprodukt von 2 Matrizen ist wie wir bereits wissen $4n^3$. Nun müssen wir uns um die Invertierung und deren Komplexität kümmern. Nach einer normalen Singulärwertzerlegung bekommen wir $O(min(mn^2,m^2 n))$. Nun müssen wir wieder die Inverse daraus herleiten. 
Oder wir können den Gauß Algorithmus oder mit der Neumann Reihe arbeiten. Gauß Algorithmus gibt uns eine Komplexität von $O(n^3)$. Neumann Reihe macht keinen Sinn, da die Komplexität mindestens genau so hoch ist wie bei Gauß.
Insgesamt haben wir eine Komplexität mit Gauß: $O(n^3)+O(n^3)+O(4n^3)=O(6n^3) \in O(n^3)$. Das heißt wir sind immer noch kubisch, was ein großer Vorteil ist.
Die Singulärwertzerlegung bringt uns erst einen Vorteil, wenn wir eine trunkierte Zerlegung wählen. Die mögliche Ersparnis ist aber nicht so hoch. \textbf{BEGRÜNDUNG}.

\textbf{Singulärwertzerlegung höherer Ordnung}
Nun haben wir uns mit Hilfe der Tucker Dekomposition eine Herleitung für die Pseudoinverse erarbeitet. Nun geht es um die effiziente Berechnung dieser Formel. Dazu wollen wir uns die Summenfaktorisierung zu nutze machen, wie sie auch in \cite[9-11]{Teachlet} vorgeschlagen wird.
Sei $\mathcal{A} \in \mathbb{R}^{I_{1} \times \dots \times I_{n}}$ .
Die Formel für die Pseudoinverse lautet:

\begin{equation} \label{eq:pinv}
\mathcal{A}^{\dagger} = \mathcal{S}^{\dagger} \times_{n=1}^{N} U^{ (n) ^{T} }
\end{equation}

Wobei $\mathcal{S} \in \mathbb{R}^{I_{1} \times \dots \times I_{n}}$ und $U^{(n)} \in \mathbb{R}^{J_{n} \times I_{n}}$
Man kann \ref{eq:pinv} nach \cite[462]{Kolda} äquivalent umformen zu

\begin{equation}
\begin{aligned}
\mathcal{A}^{\dagger}_{(n)}  &= U^{ (n) ^{T} }  \mathcal{S}^{\dagger}_{(n)} ( U^{ (N) ^{T} } \otimes \dots \otimes U^{ (n+1) ^{T} } \otimes U^{ (n-1) ^{T} } \otimes \dots \otimes U^{ (1) ^{T} })^{T} \\ \iff
\mathcal{A}^{\dagger}_{(n)} &= U^{ (n) ^{T} }  \mathcal{S}^{\dagger}_{(n)} ( U^{ (N)  } \otimes \dots \otimes U^{ (n+1) } \otimes U^{ (n-1) } \otimes \dots \otimes U^{ (1) })
\end{aligned}
\end{equation}

Nun betrachten wir uns das Matrix-Vektor Produkt und überlegen uns wie wir uns die Strukturen dort zu nutze machen.

\begin{equation} \label{eq:pinvv}
\begin{aligned}
\mathcal{A}^{\dagger}_{(n)}v&= U^{ (n) ^{T} }  \mathcal{S}^{\dagger}_{(n)} ( U^{ (N)  } \otimes \dots \otimes U^{ (n+1) } \otimes U^{ (n-1) } \otimes \dots \otimes U^{ (1) }) v
\end{aligned}
\end{equation}

Wir schauen uns die Struktur mal für den Fall, dass $\mathcal{A} \in \mathbb{R}^{I_{1} \times \dots \times I_{4}}$. Das heißt \ref{eq:pinvv} reduziert sich auf:

\begin{equation} \label{eq:pinvcase}
\begin{aligned}
\mathcal{A}^{\dagger}_{(n)}v&= U^{ (n) ^{T} }  \mathcal{S}^{\dagger}_{(n)} ( U^{ (N_{1})  } \otimes U^{ (N_{2})}  \otimes U^{ (N_{3}) }) v
\end{aligned}
\end{equation}
mit $N_{i} \neq n$.

Wir erkennen auf der Rechten seite können wir unsere Methodik, die wir entwickelt haben, um dieses doppelte Kronecker Produkt mit dem Vektor effizient zu berechnen. 
Wir können dies mit einer Komplexität von $O(6n^3)$ machen. Nun geht es darum den Kerntensor zu invertieren. Wie wir wissen müssen wir dazu erstmal kleine Zahlen, die wir Rauschen nennen, auf 0 setzen und die Diagonalelemente invertieren. Dazu müssen wir, da der Kerntensor vollbesetzt ist, durch alle Elemente des Tensors durchiterieren! Das heißt wir haben eine Komplexität von $O(n^4)$, da unser Tensor Ordnung 4 hat. Nun folgt eine letzte Matrix Matrix Multiplikation, aber von Matrizen der Größe $n^2 \times n^2$. Das heißt eine Komplexität von $O(n^6)$ was uns die ganze Komplexität mit raketengeschwindigkeit zerbombt. Doch dank der Struktur des Kerntensors reduziert sich die Komplexität auf $O(n^4)$. 
Insgesamt haben wir eine Komplexität von $O(6n^3)+2O(n^4)=O(6n^3+2n^4)$. Dies ist aber nicht annähernd so gut wie Option 1. Wie können wir unsere effizienz weiter steigern?
Wir können uns an die trunkierte HOSVD ranmachen oder eine andere Alternative finden.
Nun anstatt den Kerntensor zu modifizieren, können wir bei der Matrix-Matrix Multiplikation einfach nur die Diagonalelemente für die Multiplikation in Erwägung ziehen. Damit sparen wir uns schon $O(n^4)$ Operationen. Was wir tun könnten ist den Kerntensor in das Kronecker Produkt reinmultiplizieren bevor wir dieses ausrechnen. Selbst wenn wir das hinkriegen würden wir es nicht hinbekommen die Komplexität von Option 1 zu übertreffen. Außerdem ist das Problem, dass wir nicht mit der Pseudoinverse eine Matrix-Vektor Multiplikation durchführen, sondern mit dem entfalteten Tensor, der die Pseudoinverse darstellt. Doch wie dieser Tensor entfaltet wird, ist für die korrekte Berechnung von enormer Wichtigkeit.
D.h. selbst wenn wir es schaffen dies effizient zu berechnen, ist das Ergebnis das was wir wollen?
Die Antwort ist: Es kommt drauf an. Worauf? Wie wir unser v aufsetzen. Unser v könnte so aufgebaut werden, dass dies in Übereinstimmung mit den Elementen des entfalteten Tensors übereinstimmt. Dadurch würden wir genau das erreichen was wir erreichen wollten. Doch wie gesagt, macht uns die Komplexität einen gewaltigen Schnitt durch die Rechnung.

Eine letzte Alternative zeigt wäre eben die trunkierte HOSVD.
Wir hauen eine Dimension raus und die korrespondierenden Singulärwerte raus. Am Meisten macht es Sinn natürlich die kleinsten Singulärwerte rauszuhauen. 

Wir erhalten:

\begin{equation}
\begin{aligned}
\mathcal{\textbf{A}}^{\dagger}_{(n)}v&= U^{ (n) ^{T} }  \mathcal{\textbf{S}}^{\dagger}_{(n)} ( U^{ (N_{1})  } \otimes U^{ (N_{2})} ) v
\end{aligned}
\end{equation}

DIe Komplexität dies auszurechnen beträgt für das Kronecker-Matrix Vektor Produkt $O(4n^3)$. Nun folgt ein weiterer Trick. Wir nutzen folgende Bemerkung:

\begin{Bemerkung}
Sei A eine Diagonalmatrix mit $A \in \mathbb{R}^{n^3 \times n^3}$ und $y \in \mathbb{R}^{n^3}$. Dann gilt:
\begin{equation*}
A(\mathcal{B} \otimes \mathcal{C})y = (\mathcal{B} \otimes \mathcal{C}) 
\begin{pmatrix}
a_{11} y_1 \\ \vdots \\ a_{n^3 n^3} y_n \\
\end{pmatrix}
\end{equation*}
\end{Bemerkung}

Um diese Transformation zu berechnen brauchen wir $O(n^3)$ Operationen. Insgesamt erhalten wir$ O(5n^3)$. Nun müssen wir noch das letzte Matrix-Matrix Produkt berechnen, was uns wieder alles kaputt macht.

Wenn wir noch eine Dimension rausstreichen enden wir bei:
\begin{equation}
\begin{aligned}
\mathcal{\textbf{A}}^{\dagger}_{(n)}v&= U^{ (n) ^{T} }  \mathcal{\textbf{S}}^{\dagger}_{(n)} ( U^{ (N_{1})  } ) v
\end{aligned}
\end{equation}