In Kapitel 3 haben wir uns zwei Möglichkeiten angeschaut, das Matrix-Vektor Produt zu berechnen mit der Pseudoinversen. Beide Alternativen bargen eine Tensorprodukt Struktur, in Form des Matrix-Kronecker Produkt.

Das heißt um dies effizient zu implementieren sollten wir uns Gedanken dadrüber machen, wie wir diese Struktur ausnutzen können.

\subsection{ Effizientes Matrix-Vektor Produkt }
In \cite{Teachlet} wird eine Strategie vorgestellt ein Matrix-Vektor Produkt mit Kronecker Produkt Matrizen $z=(\mathcal{B} \otimes \mathcal{A})y$ effektiv zu berechnen.

Sei $\mathcal{A} \in \mathbb{R}^{m \times n}$ und $\mathcal{B} \in \mathbb{R}^{p \times q}$. Das Kronecker Produkt dieser Matrizen kann man schreiben als, 
\begin{equation*}
\mathcal{B} \times \mathcal{A} =
\begin{pmatrix}
b_{11}\mathcal{A} & \dots  & b_{1q}\mathcal{A} \\
\vdots & \ddots & \vdots \\
b_{p1}\mathcal{A} & \dots & b_{pq}\mathcal{A} \\
\end{pmatrix}
\end{equation*}

Wir sehen, die sich wiederholende Struktur von $\mathcal{A}$. Genau diese wollen wir uns zu nutze machen. Nehmen wir an $y$ sei geordnet in der Indexierung.
\begin{equation*}
y = (y_1,y_2,\dots,y_n,\dots,\dots,y_{(q-1)n+1},y_{(q-1)n+2},\dots,y_{qn})^T
\end{equation*}

Wir denken uns nun die Faktoren $b_{ij}$ die mit $\mathcal{A}$ multipliziert werden erstmal weg. Definiere $y^{(1)}=(y_1,y_2,\dots,y_n)^T$.

\begin{equation*}
w^{(1)}=
\begin{pmatrix}
w_1 \\ \vdots \\ w_m 
\end{pmatrix}
=
\begin{pmatrix}
a_{11} & \dots & a_{1n} \\
\vdots & \dots & \vdots \\
a_{m1} & \dots & a_{mn} \\
\end{pmatrix}
\begin{pmatrix}
y_1 \\ \vdots \\ y_n
\end{pmatrix}
= \mathcal{A}y^{(1)}
\end{equation*}

Auf ähnliche Weise können wir uns $y^{(2)}=(y_{n+1},\dots,y_{2n})^{T}$ definieren und dann
\begin{equation*}
w^{(2)} = \mathcal{A}y^{(2)}
\end{equation*}

Wir führen dies so weiter und erhalten

\begin{equation*}
w=( (w^{(1)})^T , \dots, (w^{(q)})^T) \in \mathbb{R}^{mq}
\end{equation*}

Nun müssen wir die Informationen der Matrix B noch mit reinbringen. Dazu berechnen wir wie in \cite{Teachlet} vorgeschlagen $z_i$ mit

\begin{equation*}
z_i = \sum_{i=1}^{q} b_{1i} w_m^{(i)}
\end{equation*}

Mit Hilfe diesen Algorithmus haben wir die Komplexität von des Auswertens der Gleichung von $2m^4$ auf $4m^3$ reduziert.

\subsubsection{Erweiterung}
Dieser Algorithmus ist optimal um für die erste Alternative mit der Pseudoinversen Berechnung durch Ausnutzung der Tensorstruktur, effektiv das Matrix-Vektor Produkt auszurechnen. Doch für die zweite Alternative mit der HOSVD, werden wir eine erweiterte Form des Algorithmuses brauchen, da wir mehrere Kronecker Produkte bekommen werden. Wir wollen den Algorithmus erweitern für die effektive Berechnung von $z=(\mathcal{C} \otimes \mathcal{B} \otimes \mathcal{A})v$ mit $\mathcal{A} \in \mathbb{R}^{n_1 \times m_1}$, $\mathcal{B} \in \mathbb{R}^{n_2 \times m_2}$,$\mathcal{C} \in \mathbb{R}^{n_3 \times m_3}$ und $v \in \mathbb{R}^{m_1 m_2 m_3}$.


\begin{equation} z=
\begin{pmatrix}
c_{11} b_{11} \mathcal{A} & \dots  & c_{11} b_{1m_2} \mathcal{A} & \dots & \dots & c_{1m_3}b_{11}\mathcal{A} & \dots & c_{1m_3}b_{1m_2}\mathcal{A}  \\

\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{11} b_{n_2 1} \mathcal{A} & \dots  & c_{11} b_{n_2 m_2} \mathcal{A} & \dots & \dots & c_{1 m_3}b_{n_2 1}\mathcal{A} & \dots & c_{1 m_3}b_{n_2 m_2}\mathcal{A}  \\
c_{21} b_{n1} \mathcal{A} & \dots  & c_{21} b_{nn} \mathcal{A} & \dots & \dots & c_{2n}b_{n1}\mathcal{A} & \dots & c_{2 m_3}b_{n_2 m_2}\mathcal{A}  \\
\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{n_3 1} b_{n_2 1} \mathcal{A} & \dots  & c_{n_3 1} b_{n_2 m_3} \mathcal{A} & \dots & \dots & c_{n_3 m_3}b_{n_2 1}\mathcal{A} & \dots & c_{n_3 m_3}b_{n_2 m_2}\mathcal{A}  \\
\end{pmatrix} \, \, v
\end{equation}

Wir sehen hier sich zwei wiederholende Strukturen, die wir ausnutzen wollen um Operationen zu sparen.

\begin{equation} \label{eq:matrix} z=
\begin{pmatrix}
c_{11} \textcolor{green}{b_{11}} \textcolor{red}{\mathcal{A}} & \dots  & c_{11} \textcolor{green}{b_{1n}} \textcolor{red}{\mathcal{A}} & \dots & \dots & c_{1n}\textcolor{green}{b_{11}}\textcolor{red}{\mathcal{A}} & \dots & c_{1n}\textcolor{green}{b_{1n}}\textcolor{red}{\mathcal{A}}  \\

\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{11} \textcolor{green}{b_{n1}} \textcolor{red}{\mathcal{A}} & \dots  & c_{11} \textcolor{green}{b_{nn}} \textcolor{red}{\mathcal{A}} & \dots & \dots & c_{1n}\textcolor{green}{b_{n1}}\textcolor{red}{\mathcal{A}} & \dots & c_{1n}\textcolor{green}{b_{nn}}\textcolor{red}{\mathcal{A}}  \\
c_{21} \textcolor{green}{b_{n1}} \textcolor{red}{\mathcal{A}} & \dots  & c_{21} \textcolor{green}{b_{nn}} \textcolor{red}{\mathcal{A}} & \dots & \dots & c_{2n}\textcolor{green}{b_{n1}}\textcolor{red}{\mathcal{A}} & \dots & c_{2n}\textcolor{green}{b_{nn}}\textcolor{red}{\mathcal{A}}  \\
\vdots & \vdots & \ddots & \ddots  & \ddots & \vdots & \vdots & \vdots \\
c_{n1} \textcolor{green}{b_{n1}} \textcolor{red}{\mathcal{A}} & \dots  & c_{n1} \textcolor{green}{b_{nn}} \textcolor{red}{\mathcal{A}} & \dots & \dots & c_{nn}\textcolor{green}{b_{n1}}\textcolor{red}{\mathcal{A}} & \dots & c_{nn}\textcolor{green}{b_{nn}}\textcolor{red}{\mathcal{A}}  \\
\end{pmatrix} \, \, v
\end{equation}


Unser $v$ können wir zu einem Tensor $\mathcal{V} \in \mathbb{R}^{m_1 \times m_2 \times m_3} $ umdefinieren, damit dieser handlicher wird. Der erste Index repräsentiert in welcher Spalteneintrag von $C$, der Zweite in welchem Spalteneintrag von $B$ und der Dritte in welchem Spalteneintrag von $A$ wir uns befinden. Dies kann man wieder als eine Index Transformation ansehen, die bestimmte Einträge von $v$ in bestimmte Tensorelemente abbildet. 
Wir schauen uns erstmal die einzelnen Einträge von $z$ an. 

\begin{equation} \label{eq:zold}
\begin{aligned}
z_{1} = \mathcal{V}(1,1,1) c_{11} \textcolor{green}{b_{11}} \textcolor{red}{a_{11}}+ \dots +  \mathcal{V}(1,1,n) c_{11} \textcolor{green}{b_{11}} \textcolor{red}{a_{1n}} + \dots  +  \mathcal{V}(1,n,1)c_{11} 
\textcolor{green}{b_{1n}} \textcolor{red}{a_{11}} \\ + \dots +  \mathcal{V}(n,1,1) c_{1n} \textcolor{green}{b_{11}} \textcolor{red}{a_{11}} + \dots +  \mathcal{V}(n,n,n) c_{1n} \textcolor{green}{b_{1n}} \textcolor{red}{a_{1n}}
\end{aligned}
\end{equation}
Definiere $w_{\textcolor{red}{1}}(i,j) :=\mathcal{V}(i,j,1) a_{\textcolor{red}{1}1}+\dots+\mathcal{V}(i,j,n) a_{\textcolor{red}{1}n}$. Dann erhalten wir
\begin{equation*}
\begin{aligned}
z_{1}= w_1(1,1) c_{11} b_{11} + \dots +   w_1(1,n) c_{11} b_{1n} + \dots + w_1(n,1) c_{1n} b_{11}  + \dots +  w_1(n,n) c_{1n} b_{1n}.
\end{aligned}
\end{equation*}

Damit haben wir uns die sich wiederholende Struktur von der Matrix $\textcolor{red}{\mathcal{A}}$ zu nutze gemacht. Im nächsten Schritt machen wir uns die sich wiederholende Struktur von $\textcolor{green}{b_{ij}}$ zu nutze.
Wir definieren hierfür $\mathcal{W}_{\textcolor{red}{1},k} (i):= w_k(i,1) b_{\textcolor{red}{1}1} + \dots + w_k(i,n) b_{\textcolor{red}{1}n}$. Damit erhalten wir
\begin{equation} \label{eq:znew}
\begin{aligned}
z_{1}= \mathcal{W}_{1,1}(1) c_{11}  + \dots +  \mathcal{W}_{1,1}(n) c_{1n}.
\end{aligned}
\end{equation}

Wir wollen nun $z$ genau so umformen wie wir das auch für $v$ gemacht haben. Damit erhalten wir für allgemeines $z_{i}$ folgende Formel
\begin{equation}
\mathcal{Z}(i,j,k) = \mathcal{W}_{j,k}(1) c_{i1}  + \dots +  \mathcal{W}_{j,k}(n) c_{in} 
\end{equation}
Wobei j und k den Zeilen jeweils in den Matrizen $B$ und $C$ entsprechen. 
\newpage
Der komplette Algorithmus sieht wie folgt aus.
\begin{mdframed}[backgroundcolor=blue!3] 
\begin{algorithmic}
\For {k=1 < n }
	\For {i= 1 < n }
		\For {j= 1 < n }
			\State $w_{k}(i,j) = \mathcal{V}(i,j,1)a_{k1} + \dots + \mathcal{V}(i,j,n)a_{kn}$
		\EndFor
	\EndFor
\EndFor
\For {k=1 < n }
	\For {i= 1 < n }
		\For {j= 1 < n }
			\State $\mathcal{W}_{i,j} (k):= w_k(i,1) b_{11} + \dots + w_k(i,n) b_{1n}$
		\EndFor
	\EndFor
\EndFor
\For {k=1 < n }
	\For {i= 1 < n }
		\For {j= 1 < n }
			\State $\mathcal{Z}(i,j,k) = \mathcal{W}_{j,k}(1) c_{i1}  + \dots +  \mathcal{W}_{j,k}(n) c_{in}$ 
		\EndFor
	\EndFor
\EndFor
\end{algorithmic}
\end{mdframed}

 
Wir haben in \ref{eq:matrix} eine Matrix-Vektor Multiplikation von einer Matrix der Größe $n_1 n_2 n_3 \times m_1 m_2 m_3$. Dementsprechend hätten wir $(n_1 n_2 n_3)^{2}$ Multiplikationen und $(m_1 m_2 m_3)^{2}$ Additionen. Sei $n_M = max(n_1,n_2,n_3)$ und $m_M=max(m_1,m_2,m_3)$. Die Komplexitätsklasse ist dann $O(n_M^6+m_M^6)$. Die Komplexität des vorgeschlagenen Algorithmuses reduziert die Operationen auf $3n^{4}$ Multiplikationen und genau so viele Additionen.
Ein enorme Reduktion, vor allem für großes $n$.
\newline

\newpage
\subsection{Anwendung}

Wie können wir uns nun diese Algorithmen zu nutze machen für die in Kapitel 3 besprochenen Strategien?

\subsubsection{Summenfaktorisierung}

Aus der Tensorsturktur aus Kapitel 3 für die lokale Massematrix und die Elementsteifigkeitsmatrix der Laplace Bilinearform, haben wir die Pseudoinversen hergeleitet. Nun schauen wir uns das Matrix-Vektor Produkt mit den Pseudoinversen als Matrix und einen beliebigen Vektor $u$.


\textbf{Masse Matrix} 
\begin{equation*}
M^{+}u =  [(\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T)^+ \otimes (\mathcal{W}_N^{1D} (\mathcal{N}_{1D})^T)^+]u.
\end{equation*}

\textbf{Laplace Bilinearform}
\begin{equation*}
V^{+}u =  [\dfrac{1}{2} ((\widehat{\mathcal{W}}_N^{1D} (\widehat{\mathcal{N}}^{1D})^T)^+ \otimes (\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T)^+)]u
\end{equation*}

Wie komplex ist es die Pseudoinversen der Matrizen $(\mathcal{W}_N^{1D} (\mathcal{N}^{1D})^T)$ und $(\widehat{\mathcal{W}}_N^{1D} (\widehat{\mathcal{N}}^{1D})^T)$ zu berechnen?

Wir können als Ansatz eine Singulärwertzerlegung wählen. Aus der Singulärwertzerlegung müssen wir noch die Pseudoinverse erst herleiten.
Allgemein gilt
\begin{equation*}
M\,=\,U\Sigma V^{T}
\end{equation*}
für eine eine $m \times n$-Matrix $M$ mit Rang $r$, wobei
$U$ eine orthogonale $m\times m$-Matrix ist, $V^{T}$ die Transponierte einer orthogonalen $n\times n$-Matrix $V$ und
$\Sigma$  eine reelle  $m\times n$-Matrix mit den $r$ Singulärwerten in der Diagonalen und sonst Nullen.

Die Pseudoinverse daraus hergeleitet ergibt
\begin{equation*}
M^{+}=V\Sigma ^{+}U^{T}.
\end{equation*}

Die Komplexität für die Herleitung der Pseudoinversen aus der Singulärwertzerlegung ist vernachlässigbar. Für die Berechnung von $\Sigma^+$ haben wir $r$ Operationen, da in $\Sigma$ in der Diagonalen $r$ Einträge stehen, die wir einfach nur invertieren müssen. Die Berechnung von $U^{T}$ ist vernachlässigbar, da man bei der Berechnung der Singulärwertzerlegung direkt $U^{T}$ speichern kann anstatt $U$.
Also bekommen wir mit der Singulärwertzerlegung einer Matrix $M \in \mathbb{R}^{m \times n}$ die Pseudoinverse mit einer Komplexität von $O(min(mn^2,m^2 n))$ \cite[2]{SVD}. Man könnte natürlich hier auch approximative Verfahren wählen und versuchen an Operationen zu sparen. Ich verweise hier auf \cite{SVD} für die nähere Betrachtung von schnelleren Singulärwertzerlegungen.


Wir können stattdessen mit dem Gauß Algorithmus oder mit der Neumann Reihe arbeiten. Gauß Algorithmus gibt uns eine Komplexität von $O(n^3)$. Das Problem ist jedoch, dass die Matrizen allgemein nicht invertierbar sind. 
Neumann Reihe macht keinen Sinn, da die Komplexität mindestens genau so hoch ist wie bei Gauß.

\begin{itemize}
\item Um das Matrix-Vektor Produkt effizient zu berechnen nutzen wir einfach den Algorithmus für die Berechnung von $z=(\mathcal{B} \otimes \mathcal{A})y$ aus Kapitel 4.1. und erhalten eine Komplexität von $4n^3$.

\item Matrizenmultiplikation ist kubisch also $n^3$. 

\item Berechnung der Pseudoinversen des Kronecker Produkts liegt in der Komplexitätsklasse $O(n^3)$

\end{itemize}

Insgesamt haben wir eine Komplexität von $O(4n^3)+O(n^3)+O(n^3)=O(6n^3)$.
Wie wir noch weiter an Komplexität sparen können und wo wir anknüpfen können um effizienter zu werden, wird in Kapitel 5 diskutiert.

\subsubsection{Singulärwertzerlegung höherer Ordnung}
Nun haben wir uns mit Hilfe der Tucker Dekomposition eine Herleitung für die Pseudoinverse erarbeitet. Jetzt geht es um die effiziente Berechnung dieser Formel. Dazu wollen wir uns die Strategie zu nutze machen, die wir in Kapitel 4.1.1. hergeleitet haben für die effektive Berechnung von zwei Kronecker Produkten mit einem Vektor.
Es sei $\mathscr{X} \in \mathbb{R}^{I_{1} \times \dots \times I_{4}}$ ein Tensor. Der Tensor $\mathscr{X}$ könnte der Massetensor sein oder der Laplace Bilinearform Tensor.
Die Formel für die Pseudoinverse lautet

\begin{equation} \label{eq:pinv}
\mathscr{X}^{+} = \mathscr{G}^{+} \times_{n=1}^{4} U^{ (n) ^{T} }
\end{equation}

Wobei $\mathscr{G} \in \mathbb{R}^{I_{1} \times \dots \times I_{4}}$ und $U^{(n)} \in \mathbb{R}^{I_{n} \times I_{n}}$
Man kann (\ref{eq:pinv}) wie in (\ref{eq:tensortensor}) äquivalent umformen zu

\begin{equation}
\begin{aligned}
\mathscr{X}^{+}_{(n)}  &= U^{ (n) ^{T} }  \mathscr{G}^{+}_{(n)} ( U^{ (4) ^{T} } \otimes \dots \otimes U^{ (n+1) ^{T} } \otimes U^{ (n-1) ^{T} } \otimes \dots \otimes U^{ (1) ^{T} })^{T} \\ \iff
\mathscr{X}^{+}_{(n)} &= U^{ (n) ^{T} }  \mathscr{G}^{+}_{(n)} ( U^{ (4)  } \otimes \dots \otimes U^{ (n+1) } \otimes U^{ (n-1) } \otimes \dots \otimes U^{ (1) })
\end{aligned}
\end{equation}

Die Äquivalenz folgt mit Lemma (\ref{lemma:transpose}).
Im nächsten Schritt betrachten wir das Matrix-Vektor Produkt mit einem beliebigen Vektor $u_n \in \mathbb{R}^{I^N \cdots I^{n-1} \, I^{n+1} \cdots I^1}$ und überlegen wie wir uns die Strukturen dort zu nutze machen.

\begin{equation} \label{eq:pinvv}
\begin{aligned}
\mathscr{X}^{+}_{(n)}v&= U^{ (n) ^{T} }  \mathscr{G}^{+}_{(n)} ( U^{ (4)  } \otimes \dots \otimes U^{ (n+1) } \otimes U^{ (n-1) } \otimes \dots \otimes U^{ (1) }) u_n
\end{aligned}
\end{equation}

Wir führen die Variablen $N_i$ ein mit $N_i \neq n$. Damit können wir (\ref{eq:pinv}) reduzieren auf

\begin{equation} \label{eq:pinvcase}
\begin{aligned}
\mathscr{X}^{+}_{(n)}v&= U^{ (n) ^{T} }  \mathscr{G}^{+}_{(n)} ( U^{ (N_{1})  } \otimes U^{ (N_{2})}  \otimes U^{ (N_{3}) }) u_n \,.
\end{aligned}
\end{equation}
 

Wir erkennen, dass wir unsere Methodik nutzen können, die wir entwickelt haben um das doppelte Kronecker Produkt mit dem Vektor effizient zu berechnen. 
Dies ist möglich mit einer Komplexität von $O(6n^4)$.

Wie schaffe wir es den vollbesetzten Kerntensor zu invertieren bzw. die Pseudoinverse herzuleiten? Wie wir wissen müssen wir dazu erstmal Zahlen deren Betrag kleiner ist als $\epsilon > 0$ auf Null setzen und die restlichen Elemente einzeln invertieren. 
Dazu müssen wir, da der Kerntensor vollbesetzt ist, durch alle Elemente des Tensors durchiterieren. Das heißt, wir haben eine Komplexität von $O(n^4)$ mit $n=max(I_1,\dots,I_N)$, da unser Tensor Ordnung 4 hat.
Es folgt eine letzte Matrix-Matrix Multiplikation $U^{ (n) ^{T} }  \mathscr{G}^{+}_{(n)}$. Wie groß sind diese Matrizen? Die Matrix $ \mathscr{G}^{+}_{(n)}$ hat die Größe $I^n \times (I^N \cdots I^{n-1} \, I^{n+1} \cdots I^1)$. Die Matrix  $U^{ (n) ^{T} }$ hat die Größe $I_n \times I_n$.  Wir haben bei der Multiplikation dieser beiden Matrizen $(I_n)^3 (I^N \cdots I^{n-1} \, I^{n+1} \cdots I^1)$ Multiplikationen und genau so viele Additionen. Es sei $n=max(I_1,\dots,I_N)$. Dann können wir die Komplexitätsklassen durch $O(n^6)$ angeben. Die Anzahl der Operationen steigt mit raketengeschwindigkeit durch diese Matrix-Matrix Multiplikation. Doch dank der Struktur des Kerntensors reduziert sich die Komplexität auf $O(n^4)$. Das liegt daran, dass der Tensor $ \mathscr{G}^{+}$ super-diagonal ist. Damit müssen wir also pro Element der Ergebnismatrix nicht die Spalte mal Zeile rechnen, sondern einfach nur das Diagonalelement mal ein Element der Matrix. 


Insgesamt haben wir eine Komplexität von $O(6n^3)+2O(n^4)=O(6n^3+2n^4)$. Dies ist aber nicht annähernd so gut wie Option 1. Wie können wir unsere effizienz weiter steigern?
Wir können uns an die trunkierte HOSVD ranmachen oder eine andere Alternative finden.
Nun anstatt den Kerntensor zu modifizieren, können wir bei der Matrix-Matrix Multiplikation einfach nur die Diagonalelemente für die Multiplikation in Erwägung ziehen. Damit sparen wir uns schon $O(n^4)$ Operationen. Was wir tun könnten ist den Kerntensor in das Kronecker Produkt reinmultiplizieren bevor wir dieses ausrechnen. Selbst wenn wir das hinkriegen würden wir es nicht hinbekommen die Komplexität von Option 1 zu übertreffen. Außerdem ist das Problem, dass wir nicht mit der Pseudoinverse eine Matrix-Vektor Multiplikation durchführen, sondern mit dem entfalteten Tensor, der die Pseudoinverse darstellt. Doch wie dieser Tensor entfaltet wird, ist für die korrekte Berechnung von enormer Wichtigkeit.
D.h. selbst wenn wir es schaffen dies effizient zu berechnen, ist das Ergebnis das was wir wollen?
Die Antwort ist: Es kommt drauf an. Worauf? Wie wir unser v aufsetzen. Unser v könnte so aufgebaut werden, dass dies in Übereinstimmung mit den Elementen des entfalteten Tensors übereinstimmt. Dadurch würden wir genau das erreichen was wir erreichen wollten. Doch wie gesagt, macht uns die Komplexität einen gewaltigen Schnitt durch die Rechnung.

Eine letzte Alternative zeigt wäre eben die trunkierte HOSVD.
Wir hauen eine Dimension raus und die korrespondierenden Singulärwerte raus. Am Meisten macht es Sinn natürlich die kleinsten Singulärwerte rauszuhauen. 

Wir erhalten:

\begin{equation}
\begin{aligned}
\mathscr{\textbf{A}}^{+}_{(n)}v&= U^{ (n) ^{T} }  \mathscr{\textbf{S}}^{+}_{(n)} ( U^{ (N_{1})  } \otimes U^{ (N_{2})} ) v
\end{aligned}
\end{equation}

DIe Komplexität dies auszurechnen beträgt für das Kronecker-Matrix Vektor Produkt $O(4n^3)$. Nun folgt ein weiterer Trick. Wir nutzen folgende Bemerkung:

\begin{Bemerkung}
Sei A eine Diagonalmatrix mit $A \in \mathbb{R}^{n^3 \times n^3}$ und $y \in \mathbb{R}^{n^3}$. Dann gilt:
\begin{equation*}
A(\mathcal{B} \otimes \mathcal{C})y = (\mathcal{B} \otimes \mathcal{C}) 
\begin{pmatrix}
a_{11} y_1 \\ \vdots \\ a_{n^3 n^3} y_n \\
\end{pmatrix}
\end{equation*}
\end{Bemerkung}

Um diese Transformation zu berechnen brauchen wir $O(n^3)$ Operationen. Insgesamt erhalten wir$ O(5n^3)$. Nun müssen wir noch das letzte Matrix-Matrix Produkt berechnen, was uns wieder alles kaputt macht.

Wenn wir noch eine Dimension rausstreichen enden wir bei:
\begin{equation}
\begin{aligned}
\mathcal{\textbf{A}}^{+}_{(n)}v&= U^{ (n) ^{T} }  \mathcal{\textbf{S}}^{+}_{(n)} ( U^{ (N_{1})  } ) v
\end{aligned}
\end{equation}