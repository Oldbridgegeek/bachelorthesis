Wir wollen nun mit Hilfe von der Theorie zur Singulärwertzerlegung höherer Ordnung eine Theorie entwickeln, wie wir die Pseudoinverse zur Masse Matrix und zur Steifigkeitsmatrix der Laplace Bilinearform effiizient berechnen können. 
Es sei $\pmb{\mathscr{X}}  \in \mathbb{R}^{I_1 \times I_2 \times \dots \times I_N}$ ein Tensor. Dann können wir mit der HOSVD diesen Tensor zerlegen
\begin{equation}
\pmb{\mathscr{X}} = \pmb{\mathscr{G}} \times_{n=1}^{N} U^{ (n) }
\end{equation}
Wir können einen Tensor $\pmb{\mathscr{X}}$ in einen Kerntensor $\pmb{\mathscr{G}}$ und zugehörige Faktormatrizen $U$ zerlegen. Wie bekommen wir nun die Pseudoinverse zu $\pmb{\mathscr{X}}$? Was bedeutet in dem Kontext eines Tensors überhaupt Pseudoinverse?

Die Eigenschaften der Moore Penrose Pseudoinverse für Matrizen lautet
\begin{Lemma} (Moore Penrose Pseudoinverse) 
\begin{itemize}
\item $AA^{+}A=A$
\item $A^{+}AA^{+}=A^{+}$ ($A^{+}$ ist eine schwache Inverse der multiplikativen Halbgruppe)
\item $(AA^{+})^{*}=AA^{+}$ ($AA^{+}$ ist hermitisch)
\item $(A^{+}A)^{*}=A^{+}A$ ($A^{+}A$ ist hermitisch)
\end{itemize}
\end{Lemma}

Jetzt gilt es diese Eigenschaften für Tensoren zu übertragen. Da wir erstmal keine intuitive Tensor-Tensor Multiplikation haben, gilt es diese zu definieren. Diese Tensor-Tensor Multiplikation macht dann nur für unsere Anwendung einen Sinn und ist sonst zweckfrei.

Dafür sollten wir erstmal unsere Tensoren herleiten. Dies geschieht mit Hilfe der Tensorstruktur der Ansatzfunktionen.
Wir definieren den \textit{Massetensor} elementweise durch
\begin{equation} 
M_{i1,i2,j1,j2} = \int\limits_{T} \varphi_{i1} (x_1) \varphi_{i2}(x_2) \varphi_{j1} (x_1) \varphi_{j2} (x_2) \, d(x_1,x_2)
\end{equation}
und unseren \textit{lokalen Laplace Tensor}, welcher das pendant zu der Elementsteifigkeitsmatrix der Laplace Bilinearform ist, wie folgt
\begin{equation} 
V_{i1,i2,j1,j2} = \int\limits_{T} \varphi'_{i1} (x_1) \varphi_{i2}(x_2) \varphi'_{j1} (x_1) \varphi_{j2} (x_2) +
\varphi_{i1} (x_1) \varphi'_{i2}(x_2) \varphi_{j1} (x_1) \varphi'_{j2} (x_2) \, d(x_1,x_2)
\end{equation}

Diese Transformation von Matrix zu Tensor ist also eigentlich eine Abbildung die ein Indextupel $(i,j)$ eines Matrixelements auf den Indextupel eines Tensorelements $(i_1,i_2,j_1,j_2)$ abbildet. Damit wir uns eine Tensor-Tensor Multiplikation definieren, sollte uns diese Transformation klar sein.
In dieser Transformation stecken implizit zwei Transformationen. Nämlich
\begin{equation*}
\begin{aligned}
i &\rightarrow (i_1,i_2) \\
j &\rightarrow (j_1,j_2) \\
\end{aligned}
\end{equation*}

\begin{Definition} Tensor-Tensor Multiplikation $ttp(\cdot,\cdot)$

\end{Definition}

Nun ist kommt auch ein weiterer Trick den wir nutzen können. Trick kann nur mit Vorsicht genossen werden. Die Gleichheit ist mit einem Rechner nicht zu erzielen, daher wird die Bemerkung abgeschwächt und für Tensoren. Wir erhalten:
\begin{Lemma} Moore Penrose Pseudoinverse für Tensoren 
\begin{itemize}
\item $ttp(A,ttp(A^{+},A))-A < \epsilon$
\item $ttp(A^{+},ttp(A,A^{+}))-(A^{+}< \epsilon $ ($A^{+}$ ist eine schwache Inverse der multiplikativen Halbgruppe)
\item $(ttp(A,A^{+}))^{*}-ttp(A,A^{+}) < \epsilon $ ($AA^{+}$ ist hermitisch)
\item $(ttp(A^{+},A))^{*}-ttp(A^{+},A) < \epsilon $ ($A^{+}A$ ist hermitisch)
\end{itemize}
\end{Lemma}

Die Wahl des Epsilons ist hier entscheidend. Man könnte Maschinengenauigkeit wählen, doch ist für unser Zweck vielleicht zu Hoch gezielt. Letztlich wollen wir mit unserer Pseudoinversen einen Präkonditionierer bauen. Wenn wir durch die Wahl eines etwas größeren Epsilons erheblichen Aufwand sparen, sollten wir dies in Erwägunge ziehen.
Nun wissen wir, wie wir einen Tensor als Pseudoinverse klassifizieren können. Doch wie bekommen wir die Pseudoinverse?

Die Pseudoinverse setzt sich zusammen aus:
\begin{equation} \label{eq:pinv}
\mathcal{A}^{\dagger} = \mathcal{S}^{\dagger} \times_{n=1}^{N} U^{ (n) ^{T} }
\end{equation}

Da die Faktormatrizen orthogonal sind, reicht es also einfach die Transponierte zu nehmen. Das Invertieren des Kerntensors erweist sich nun aber als problematisch.
Hier ist es nützlich die Struktur des Kerntensors zu kennen. Der Kerntensor ist leider in den meisten Fälle vollbesetzt. Doch genaueres Hinsehen zeigt 2 Arten von Zahlen. Ziemlich große Zahlen von größer als 1 und ziemlich kleine Zahlen von kleiner als $10^{-10}$. 
Die kleinen Zahlen sind in diesem Fall unbrauchbar und beinhalten wenig Informationen. Doch das Auslöschen vieler kleiner Zahlen nimmt uns in der Summe vielleicht relevante Informationen.
Wir können also kleine Zahlen einfach ausradieren und erhalten plötzlich einen super-diagonalen Tensor. Die Invertierung des Tensors beschränkt sich darauf einfach jedes Diagonalelement zu Invertieren.

\textbf{Als nächstes wollen wir klären wie wir überhaupt von Massen Matrix Form und Laplace Bilinearform zu einer Tensorform kommen.}
Die Masse Matrix M besteht aus Einträgen $M_{ij}= (\phi_i,\phi_j)$, wobei $( \dot )$ die zugehörige Bilinearform ist und $\phi_k$ eine 2 dimensionale Ansatzfunktion. Dies kann man vereinfachen mit
$M_{ij}=(\phi_i,\phi_j)=(\phi_{i_1} \phi_{i_2},\phi_{j_1} \phi{j_2})=M^{*}_{i_1 i_2 j_1 j_2}$, wobei wir nun 1 dimensionale Ansatzfunktionen haben. $M^{*}$ ist ein Tensor 4.Ordnung, den wir nun mit Hilfe der Singulärwertzerlegung höherer Ordnung zerlegen können.
Für die Laplace Bilinearform ist es ähnlich, denn wir haben in schwacher Form $L_{ij}= ( \nabla \phi_i, \nabla \phi_j)=(\nabla \phi_{i_1} \nabla \phi_{i_2}, \nabla \phi_{j_1}  \nabla \phi{j_2})=L^{*}_{i1i2j1j2}$

Wir wissen nun wie wir unsere Tensoren berechnen können und wissen auch wie die Pseudoinverse sich gewinnen lässt mittles der Singulärwertzerlegung höherer Ordnung.
Der nächste Punkt ist die effiziente Berechnung der Pseudoinversen.



