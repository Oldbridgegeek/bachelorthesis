In \cite{Teachlet} wird die effektive Berechnung der Masse Matrix mit einem Vektor vorgestellt. Diese Methodik sollten wir uns kurz vor Augen führen und daraufaufbauend um einige eigene Gedanken erweitern um diese Methodik für unsere Anwendung zugänglich zu machen.
Das Ziel dieses Unterkapitels ist die Tensorstruktur für die Massematrix und der Laplace Bilinearform herzuleiten und für die Berechnung der Pseudoinversen zu nutzen. 

\subsubsection{Berechnung der Elementmassenmatrix}
Es sei $T$ die Referenzzelle für Rechtecke und $\varphi_i(\bold{x})$ Basisfunktion des diskreten Raumes $V_n$ mit $\bold{x}=(x,y)$ .
\begin{equation} \label{eq:mass}
M_{ik} = \int\limits_{T} \varphi_i (\bold{x}) \, \varphi_j (\bold{x}) \, d\bold{x}
\end{equation}

Die Basisfunktionen haben eine Tensorstruktur, die wie folgt aussieht
\begin{equation} \label{eq:tensor}
\varphi_i(\bold{x})=\varphi_{i_1+(N+1)i_2}(x,y)=\varphi_{i_1}(x)\varphi_{i_2}(y)
\end{equation}

Wir werden eine lexikographische Ordnung der Freiheitsgrade benutzen. Die Reichweite von $\, \, i_1 $ und $ i_2 \, $ reicht von $0$ bis $N$. Das heißt der Index $i$ geht von $0$ bis $N_p=(N+1)^2$. In Abbildung (\ref{fig:lexi}) sehen Sie ein Beispiel für $N=3$.

\begin{figure}[ht] 
	\centering
  \includegraphics[width=0.3\textwidth]{lexi.png}
	\caption{ \cite[3]{Teachlet}}
	\label{fig:lexi}
\end{figure}

Wir wollen für die Berechnung der Integrale in (\ref{eq:mass}) die Gauss Quadratur benutzen.
Es seien $\bold{x}_q=(x_{q1},x_{q2})$ die Stützstellen und $\bold{w}_q=w_{q1}w_{q2}$ die Gewichte. Dann können wir (\ref{eq:mass}) approximieren wie folgt

\begin{equation} \label{eq:massapprox}
\begin{aligned}
M_{ij} &= \int\limits_{T} \varphi_i (\bold{x}) \, \varphi_j (\bold{x}) \, d\bold{x} \\
&\approx  \sum\limits_{q=1}^Q \bold{w}_q \, \, \varphi_i (\bold{x}_q) \, \varphi_j (\bold{x}_q) \\
&= \sum\limits_{q_1=1}^{Q_{1D}} \sum\limits_{q_2=1}^{Q_{1D}} \varphi_{i_1}(x_{q1}) \varphi_{i_2}(x_{q2}) \varphi_{j_1}(x_{q1}) \varphi_{j_2}(x_{q2}) \, w_{q1} w_{q2}
\end{aligned}
\end{equation}

Wir wählen die Anzahl der Quadraturpunkte $Q_{1D}$ per Dimension so, dass wir exakt integrieren. Wir wählen $Q_{1D}$ gleich der Anzahl der Basisfunktionen $N+1$, da wir mit der Gauss Quadratur mit $N+1$ Stützstellen bis $2N+1$ exakt integrieren und der höchste Grad bei uns $2N$ ist.

Wir definieren uns $\mathcal{N}_{iq}=\varphi_i(x_q)$ und weiterhin die Matrix $\mathcal{W}_{ii}=\bold{w}_i$, die in der Diagonalen die Quadraturgewichte hat und sonst Nullen. Damit können wir nun die Massematrix schreiben als
\begin{equation}
M = \mathcal{N} \mathcal{W} \mathcal{N}^T
\end{equation}

Dies ist auch die Form mit in $\mathcal{N}$ zweidimensionale Basisfunktionen. Wir können dies aber weiter aufspalten in eindimensionale Basisfunktionen und dadurch eine Effizienzsteigerung erzielen bei der Berechnung des Matrix-Vektor Produkts mit der Elementmassenmatrix.
\subsubsection{Berechnung der Elementmassenmatrix-Vektor Produkt}

Das Ziel dieses Unterkapitels ist die effiziente Berechnung des Matrix-Vektor Produkts $Mu=v$ mit $M$ als Massematrix.

Wir fangen damit an, die Matrix $\mathcal{N}$ in ein Tensorprodukt aufzuspalten. 
\begin{equation} \label{eq:onedim}
\mathcal{N} = N_{1D} \otimes N_{1D}
\end{equation}

Die Matrix $\mathcal{N}_{1D}$ ist nun äquivalent definiert wie $\mathcal{N}$ bloß mit eindimensionalen Basisfunktionen. Äquivalent ausgeschrieben sieht die Gleichung (\ref{eq:onedim}) wie folgt aus

\begin{equation*}
\begin{pmatrix}
\varphi^{2D}_1(\bm{x}_1) & \hdots & \varphi^{2D}_N(\bm{x}_1) \\
\vdots & \ddots & \vdots \\
\varphi^{2D}_1(\bm{x}_Q) & \hdots & \varphi^{2D}_N(\bm{x}_Q)
\end{pmatrix}
=
\begin{pmatrix}
\varphi^{1D}_1(x_1) & \hdots & \varphi^{1D}_n(x_1) \\
\vdots & \ddots & \vdots \\
\varphi^{1D}_1(x_{Q_{1D}}) & \hdots & \varphi^{1D}_n(x_{Q_{1D}})
\end{pmatrix}
\otimes
\begin{pmatrix}
\varphi^{1D}_1(x_1) & \hdots & \varphi^{1D}_n(x_1) \\
\vdots & \ddots & \vdots \\
\varphi^{1D}_1(x_{Q_{1D}}) & \hdots & \varphi^{1D}_n(x_{Q_{1D}})
\end{pmatrix}
\end{equation*}

Im dreidimensionalen Fall wäre die Aufspaltung analog
\begin{equation}
\mathcal{N}^{3D} = N_{1D} \otimes N_{1D} \otimes N_{1D}
\end{equation}

\newpage


Angenommen wir hätten eine Lösung $u \in V$ der Form
\begin{equation}
u(x,y)=\sum_{h=1}^{n^2} \psi_h (x,y) u_h,\ u_h \in \mathbb{R} \ \forall h =1,...\ ,n^2=:N
\end{equation}
mit $\psi_h$ ist eine globale Ansatzfunktion. Diese Anstazfunktionen können als das Tensorprodukt von zwei ein dimensionalen Polynomen ausgedrückt werden. Die Überführung ist definiert mit
\begin{equation}
h=j(n-1)+i,\ i<n \leftrightarrow (i,j)
\end{equation}
was uns eine alternative Representation von u gibt
\begin{equation}
u(x,y)=\sum_{i=1}^n \sum_{j=1}^n \varphi_i(x) \varphi_j(y) u_{ij}, \ i,j =1,...\ n
\end{equation}
Seien $q=(q_1,...\ ,q_n)^T$ Quadraturpunkte einer 1-dimensionalen Quadraturegel mit zugehörigen Gewichten $w=(w_1,...\ ,w_n)^T$. Wir können die 2-dimensionale Regel erhalten indem wir wieder das Tensorprodukt ausnutzen und erhalten  $(\bm{q}_1,...\ ,\bm{q}_N)$ und Gewichten $(\bm{w}_1,...\ ,\bm{w}_N)$ mit $\bm{q}_h=(q_i,q_j)$ and $\bm{w}_h=w_i w_j$.\\
Erstmals wollen wir $u$ in jedem Quadraturpunkt evaluieren. Das können wir mit einem Matrix Vektor Produkt ausdrücken:
\begin{equation}
\begin{pmatrix}
\psi_1(\bm{q}_1) & \hdots & \psi_N(\bm{q}_1) \\
\vdots & \ddots & \vdots \\
\psi_1(\bm{q}_N) & \hdots & \psi_N(\bm{q}_N)
\end{pmatrix}
\begin{pmatrix}
u_1 \\
\vdots \\
u_N
\end{pmatrix}
=
\begin{pmatrix}
u(\bm{q}_1) \\
\vdots \\
u(\bm{q}_N)
\end{pmatrix}
\end{equation}
Die Matrix kann als Tensorprodukt zweier, in diesem Fall sogar identischer, Matrizen:
\begin{equation}
\mathcal{N}^T \otimes \mathcal{N}^T =
\begin{pmatrix}
\varphi_1(q_1) & \hdots & \varphi_n(q_1) \\
\vdots & \ddots & \vdots \\
\varphi_1(q_n) & \hdots & \varphi_n(q_n)
\end{pmatrix}
\otimes
\begin{pmatrix}
\varphi_1(q_1) & \hdots & \varphi_n(q_1) \\
\vdots & \ddots & \vdots \\
\varphi_1(q_n) & \hdots & \varphi_n(q_n)
\end{pmatrix}
\end{equation}
Sei $(\mathcal{U}_{ij}) \in \mathbb{R}^{n \times n}$ die Matrix mit den Koeffizienten $u_{ij}$ als Elemente.
Durch die Nutzung der Summenfaktorisierung können wir diese Matrizen multiplizieren anstatt der Formel in (4). Wir erhalten: 
\begin{equation}
\mathcal{N}^T \mathcal{U} \mathcal{N} = \bar{\mathcal{U}}, \ \ \ \ \bar{\mathcal{U}}_{ij} = u(q_i,q_j)
\end{equation}
Die Quadraturgewichte und die Summe über die Elemente kann in einer effiziente Weise vollzogen werden, wenn wir die Tensorprodukt Struktur ausnutzen. Die Multiplizierung $\bar{\mathcal{U}}$ mit den Quadraturgewichten $w$ von der rechten Seite wird uns die gewichteten Spalten aufsummieren, , multipliziert mit $w^T$ wird den Rest erledigen. Die ganze Operation sieht wie folgt aus: 
\begin{equation}
\int \int u(x,y) dx \ dy \approx w^T \mathcal{N}^T \mathcal{U} \mathcal{N} w
\end{equation} 

Wir haben nun $u$ integriert. Jedoch können wir nun mit Ansatzfunktionen testen ohne viel zu verändern.
Wir schauen uns $f(x,y)=g(x)h(y)$ an. Also ist $f$ seperabal und somit müssen wir nur den letzten Schritt ändern. Anstatt nur den Gewichten zu multiplizieren, werden wir auch mit den Ansatzfunktionen, an den bestimmten Quadraturpunkten evaluiert, multiplizieren:

\[\bar{w}^x=(w_1 g(q_1), ... \ ,w_n g(q_n))^T \ \ \ \ \ \ \ \bar{w}^y=(w_1 h(q_1), ... \ ,w_n h(q_n))^T\]
\begin{equation}
\int \int u(x,y) f(x,y) dx \ dy \approx (\bar{w}^x)^T \mathcal{N}^T \mathcal{U} \mathcal{N} \bar{w}^y
\end{equation}

Wenn wir einen Menge von Ansatzfunktionen haben, welche hergeleitet sind von dem Tensorprodukt von identischen 1-dimensionalen Ansatzfunktionen, können wir die Matrizen multiplizieren anstatt die Vektoren im letzten Schritt, was nun die Operation vollständig macht. Wir definieren:

\[W=
\begin{pmatrix}
w1 \varphi_1(q_1) & \hdots & w_n \varphi_1(q_n) \\
\vdots & \ddots & \vdots \\
w1 \varphi_n(q_1) & \hdots & w_n \varphi_n(q_n)
\end{pmatrix}
\]

Da  $\mathcal{W} \mathcal{N}^T$ auf Beiden Seiten auftaucht, können wir diese Operationr mit 3 Matrixmultiplikationen ausdrücken. Finale Form der Gleichung:
\begin{equation}
\mathcal{V} = \mathcal{W} \mathcal{N}^T \mathcal{U} (\mathcal{W} \mathcal{N}^T)^T
\end{equation}
Die Formel leicht geändert kann benutzt werden um andere Bilinearformen auszudrücken:

$(\nabla u, \nabla v)$.
\begin{align}
(\nabla u, \nabla v) &=(\partial_x u,\partial_x v) + (\partial_y u,\partial_y v) \\
					   &=\sum_{i,j} u_{ij} (\varphi_i'(x)\varphi_j(y),\phi'(x)\phi(y)) \\
					   & \ + \sum_{i,j} u_{ij} (\varphi_i(x)\varphi_j'(y),\phi(x)\phi'(y))
\end{align}

Nun werden wir 2 Matrizen  $\mathcal{W}'$ und $\mathcal{N}'$ vorstellen.
Diese Matrizen sind ähnlich zu den oben, aber anstatt die Ansatzfunktionen zu evaluieren, evaluieren sie die Ableitung der Ansatzfunktionen. Die resultierende Form ergibt sich aus:
\[\mathcal{V}=\mathcal{W}' \mathcal{N}'^T \mathcal{U} (\mathcal{W} \mathcal{N}^T)^T +\mathcal{W} \mathcal{N}^T \mathcal{U} (\mathcal{W}' \mathcal{N}'^T)^T\]
Sei $(-\Delta u, v)=-(\partial_{xx} u,v) + -(\partial_{yy} u,v)$.\\
Wir führen $\mathcal{N}''$ ein, was die Werte der 2.Ableitung der Ansatzfunktionen in den Quadraturgewichten speichert. Bemerke, dass die Matrix $\mathcal{W}$ nicht modifziert wurde. Als Resultat erhalten wir
\[\mathcal{V}=-\mathcal{W} \mathcal{N}''^T \mathcal{U} (\mathcal{W} \mathcal{N}^T)^T -\mathcal{W} \mathcal{N}^T \mathcal{U} (\mathcal{W} \mathcal{N}''^T)^T\]


Zusammengefasst erhalten wir für die Masse Matrix die Form
\begin{equation}
M*u = (\mathcal{W} \mathcal{N}^{T}) \otimes (\mathcal{W} \mathcal{N}^{T})*u
\end{equation}

und für die Laplace Bilinearform

\begin{equation}
\mathcal{V}*u = -((\mathcal{W} \mathcal{N''}^{T}) \otimes (\mathcal{W} \mathcal{N}^{T}) - (\mathcal{W} \mathcal{N}^{T}) \otimes (\mathcal{W} \mathcal{N''}^{T}))*u
\end{equation}

DIe Berechnung der Pseudoinversen passiert mit diesem Ansatz quasi on the fly, da wir hier gleich als Ergebnis nicht die Pseudoinverse bekommen, sondern das Matrix Vektor Produkt mit der Pseudoinversen als Matrix.

Für die effiziente Berechnung der Inversen brauchen wir folgendes Ergebnis:
\begin{Bemerkung}
\begin{equation}
(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}
\end{equation}
\end{Bemerkung}

Nun erhalten wir fast direkt unser gewünschtes Ergebnis
\begin{equation}
M^{-1}*u = ((\mathcal{W} \mathcal{N}^{T}) \otimes (\mathcal{W} \mathcal{N}^{T}))^{-1}*u
= ((\mathcal{W} \mathcal{N}^{T})^{-1} \otimes (\mathcal{W} \mathcal{N}^{T})^{-1})*u
\end{equation}

$\mathcal{W} \mathcal{N}^{T} \in \mathbb{R}^{n \times n}$ somit also quadratisch. Das heißt um das Pseudoinverse-Vektor Produkt zu berechnen müssen wir nur eine SVD von einer $n \times n$ Matrix berechnen. Falls uns das zu teuer wird können wir auch eine truncated SVD benutzen. In welchem Komplexitätsbereich wir uns befinden, wird im nächsten Subkapitel angeführt.

Für die Laplace Bilinearform ähnlich:
\begin{equation}
\mathcal{V}^{-1}*u = -((\mathcal{W} \mathcal{N''}^{T}) \otimes (\mathcal{W} \mathcal{N}^{T}) - (\mathcal{W} \mathcal{N}^{T}) \otimes (\mathcal{W} \mathcal{N''}^{T}))^{-1}*u
= -((\mathcal{W} \mathcal{N''}^{T})^{-1} \otimes (\mathcal{W} \mathcal{N}^{T})^{-1} - (\mathcal{W} \mathcal{N}^{T})^{-1} \otimes (\mathcal{W} \mathcal{N''}^{T})^{-1})*u
\end{equation}

Praktisch müssen wir also nur die Inversen von $(\mathcal{W} \mathcal{N''}^{T})$ und von $(\mathcal{W} \mathcal{N}^{T})$ berechnen.

Nun war dies der 2 Dimensionale Fall. Die Erweiterung auf den 3 Dimensionalen Fall ist analog und kann im ``Efficient Evaluation ..... Teachlet'' angeschaut werden.

Zu der Implementierungsstrategie kann man in Kapitel 4 mehr erfahren.






